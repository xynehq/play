{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "445336c6",
   "metadata": {},
   "source": [
    "\n",
    "# ðŸš€ CPT/DAPT + Tiny Anchor with QLoRA (Minimal Notebook)\n",
    "This notebook scaffolds **Domain-Adaptive Pretraining (CPT/DAPT)** on your docs using **QLoRA**, \n",
    "mixed with a **small instruction anchor (5â€“10%)** to preserve instruction-following.\n",
    "\n",
    "**Steps**: chunk docs â†’ build CPT dataset â†’ (what is a collator) â†’ interleave CPT+anchor â†’ load model â†’ train â†’ guardrails â†’ eval.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd14f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # If on Colab/fresh VM, uncomment to install:\n",
    "# !pip install -U transformers accelerate peft datasets bitsandbytes sentencepiece einops trl tensorboard\n",
    "\n",
    "import os, re, json, math, glob, random\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "import torch\n",
    "from datasets import load_dataset, interleave_datasets\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, Trainer, TrainingArguments\n",
    "from transformers.trainer_callback import EarlyStoppingCallback\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "CFG = {\n",
    "    \"run_name\": \"dapt_gemma3_v0\",\n",
    "    \"model_name\": \"google/gemma-3-27b-it\",  # or base if available\n",
    "    \"block_size\": 2048,\n",
    "    \"pack_factor\": 4,\n",
    "    \"cpt_weight\": 0.9,\n",
    "    \"anchor_weight\": 0.1,\n",
    "    \"epochs\": 1,\n",
    "    \"lr\": 1.0e-4,\n",
    "    \"warmup_ratio\": 0.03,\n",
    "    \"per_device_train_batch_size\": 1,\n",
    "    \"gradient_accumulation_steps\": 16,\n",
    "    \"eval_steps\": 250,\n",
    "    \"save_steps\": 500,\n",
    "    \"logging_steps\": 25,\n",
    "    \"seed\": 42,\n",
    "    \"use_bf16\": True,\n",
    "    \"gradient_checkpointing\": True,\n",
    "    \"output_dir\": \"./outputs_dapt\",\n",
    "    \"raw_dir\": \"./data/raw\",\n",
    "    \"cpt_jsonl\": \"./data/processed/cpt.jsonl\",\n",
    "    \"anchor_jsonl\": \"./data/processed/anchor_instr.jsonl\",\n",
    "    \"lora_r\": 16,\n",
    "    \"lora_alpha\": 32,\n",
    "    \"lora_dropout\": 0.05,\n",
    "    \"target_modules\": [\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"],\n",
    "}\n",
    "os.makedirs(\"./data/processed\", exist_ok=True)\n",
    "os.makedirs(CFG[\"output_dir\"], exist_ok=True)\n",
    "\n",
    "random.seed(CFG[\"seed\"])\n",
    "torch.manual_seed(CFG[\"seed\"])\n",
    "\n",
    "print(\"Config loaded:\", CFG)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dae4f7f",
   "metadata": {},
   "source": [
    "\n",
    "## What is a *collator*?\n",
    "A **collator** creates a uniform batch from variable-length items: it **pads** sequences, builds the **attention mask**, \n",
    "and ensures **labels** align with `input_ids`. For **CPT**, we set `labels == input_ids` (except padding set to -100) \n",
    "so the model learns next-token prediction on every token.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b05c0bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Build CPT dataset from ./data/raw ---\n",
    "def read_texts_from_dir(raw_dir, exts=(\".txt\", \".md\")):\n",
    "    paths = []\n",
    "    for ext in exts:\n",
    "        paths += glob.glob(os.path.join(raw_dir, f\"**/*{ext}\"), recursive=True)\n",
    "    texts = []\n",
    "    for p in paths:\n",
    "        try:\n",
    "            with open(p, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "                t = f.read()\n",
    "                t = re.sub(r\"[ \\t]+\\n\", \"\\n\", t)\n",
    "                t = re.sub(r\"\\n{3,}\", \"\\n\\n\", t)\n",
    "                texts.append(t.strip())\n",
    "        except Exception as e:\n",
    "            print(\"Skipping\", p, e)\n",
    "    return texts\n",
    "\n",
    "def naive_paragraph_chunk(text, max_chars=4000, min_chars=1200):\n",
    "    paras = [p.strip() for p in text.split(\"\\n\\n\") if p.strip()]\n",
    "    chunks, buf, size = [], [], 0\n",
    "    for p in paras:\n",
    "        if size + len(p) > max_chars and size >= min_chars:\n",
    "            chunks.append(\"\\n\\n\".join(buf)); buf, size = [], 0\n",
    "        buf.append(p); size += len(p) + 2\n",
    "    if buf: chunks.append(\"\\n\\n\".join(buf))\n",
    "    return chunks\n",
    "\n",
    "def make_cpt_jsonl_from_dir(raw_dir, out_jsonl):\n",
    "    texts = read_texts_from_dir(raw_dir)\n",
    "    count = 0\n",
    "    with open(out_jsonl, \"w\", encoding=\"utf-8\") as f:\n",
    "        for t in texts:\n",
    "            for ch in naive_paragraph_chunk(t):\n",
    "                if len(ch) < 200: \n",
    "                    continue\n",
    "                f.write(json.dumps({\"text\": ch}, ensure_ascii=False) + \"\\n\")\n",
    "                count += 1\n",
    "    print(f\"Wrote {count} CPT chunks -> {out_jsonl}\")\n",
    "\n",
    "# Demo file if nothing exists\n",
    "if not Path(CFG[\"cpt_jsonl\"]).exists():\n",
    "    os.makedirs(CFG[\"raw_dir\"], exist_ok=True)\n",
    "    with open(\"./data/raw/demo.txt\", \"w\") as f:\n",
    "        f.write((\"Screening Service...\\n\" * 50))\n",
    "    make_cpt_jsonl_from_dir(CFG[\"raw_dir\"], CFG[\"cpt_jsonl\"])\n",
    "else:\n",
    "    print(\"Found CPT jsonl:\", CFG[\"cpt_jsonl\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5749ddde",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Tiny anchor (Alpaca-like) ---\n",
    "def write_demo_anchor(out_jsonl):\n",
    "    demo = [\n",
    "        {\"instruction\": \"Explain the Screening Service in one paragraph.\", \"input\":\"\", \"output\":\"It performs low-latency fraud checks by querying a local cache...\"},\n",
    "        {\"instruction\": \"List two benefits of local cache screening.\", \"input\":\"\", \"output\":\"Lower latency and resilience to transient central outages.\"}\n",
    "    ]\n",
    "    with open(out_jsonl, \"w\", encoding=\"utf-8\") as f:\n",
    "        for r in demo:\n",
    "            f.write(json.dumps(r, ensure_ascii=False) + \"\\n\")\n",
    "    print(\"Wrote anchor demo ->\", out_jsonl)\n",
    "\n",
    "if not Path(CFG[\"anchor_jsonl\"]).exists():\n",
    "    write_demo_anchor(CFG[\"anchor_jsonl\"])\n",
    "else:\n",
    "    print(\"Found anchor jsonl:\", CFG[\"anchor_jsonl\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f1ede3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Tokenizer & pack helpers ---\n",
    "tokenizer = AutoTokenizer.from_pretrained(CFG[\"model_name\"], use_fast=True)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "def group_texts(examples, block_size):\n",
    "    concatenated = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    total_len = len(concatenated[\"input_ids\"])\n",
    "    total_len = (total_len // block_size) * block_size\n",
    "    result = {k: [t[i:i+block_size] for i in range(0, total_len, block_size)] for k, t in concatenated.items()}\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result\n",
    "\n",
    "def load_cpt_dataset(jsonl_path, block_size=2048, add_eos=True):\n",
    "    ds = load_dataset(\"json\", data_files=jsonl_path, split=\"train\")\n",
    "    max_len = CFG[\"block_size\"] * CFG[\"pack_factor\"]\n",
    "    def tok_fn(batch):\n",
    "        texts = batch[\"text\"]\n",
    "        if add_eos and tokenizer.eos_token:\n",
    "            texts = [t + tokenizer.eos_token for t in texts]\n",
    "        out = tokenizer(texts, truncation=True, max_length=max_len, add_special_tokens=False)\n",
    "        return out\n",
    "    tokenized = ds.map(tok_fn, batched=True, remove_columns=ds.column_names)\n",
    "    tokenized = tokenized.map(lambda e: group_texts(e, block_size), batched=True)\n",
    "    return tokenized\n",
    "\n",
    "def build_anchor_prompt(rec):\n",
    "    instr = rec.get(\"instruction\",\"\").strip()\n",
    "    inpt = rec.get(\"input\",\"\").strip()\n",
    "    if inpt:\n",
    "        return f\"[INST] {instr}\\n{inpt} [/INST]\\n\"\n",
    "    return f\"[INST] {instr} [/INST]\\n\"\n",
    "\n",
    "def load_anchor_dataset(jsonl_path):\n",
    "    ds = load_dataset(\"json\", data_files=jsonl_path, split=\"train\")\n",
    "    def tok_map(batch):\n",
    "        prompts = [build_anchor_prompt(r) for r in batch]\n",
    "        outs = [r.get(\"output\",\"\") for r in batch]\n",
    "        prompt_tok = tokenizer(prompts, add_special_tokens=False)\n",
    "        out_tok = tokenizer(outs, add_special_tokens=False)\n",
    "        input_ids, labels = [], []\n",
    "        for p_ids, o_ids in zip(prompt_tok[\"input_ids\"], out_tok[\"input_ids\"]):\n",
    "            ids = p_ids + o_ids + ([tokenizer.eos_token_id] if tokenizer.eos_token_id is not None else [])\n",
    "            lab = [-100]*len(p_ids) + o_ids + ([tokenizer.eos_token_id] if tokenizer.eos_token_id is not None else [])\n",
    "            input_ids.append(ids); labels.append(lab)\n",
    "        return {\"input_ids\": input_ids, \"labels\": labels}\n",
    "    tokenized = ds.map(tok_map, batched=True, remove_columns=ds.column_names)\n",
    "    tokenized = tokenized.map(lambda e: group_texts(e, CFG[\"block_size\"]), batched=True)\n",
    "    return tokenized\n",
    "\n",
    "cpt_ds = load_cpt_dataset(CFG[\"cpt_jsonl\"], block_size=CFG[\"block_size\"])\n",
    "anchor_ds = load_anchor_dataset(CFG[\"anchor_jsonl\"])\n",
    "train_ds = interleave_datasets([cpt_ds, anchor_ds], probabilities=[CFG[\"cpt_weight\"], CFG[\"anchor_weight\"]], seed=CFG[\"seed\"])\n",
    "\n",
    "print(\"CPT examples:\", len(cpt_ds))\n",
    "print(\"Anchor examples:\", len(anchor_ds))\n",
    "print(\"Interleaved length:\", len(train_ds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f3484c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Collator ---\n",
    "@dataclass\n",
    "class DataCollatorForCausalPairs:\n",
    "    tokenizer: Any\n",
    "    def __call__(self, features: List[Dict[str, Any]]) -> Dict[str, torch.Tensor]:\n",
    "        input_ids = [torch.tensor(f[\"input_ids\"], dtype=torch.long) for f in features]\n",
    "        labels = [torch.tensor(f[\"labels\"], dtype=torch.long) for f in features]\n",
    "        input_ids = torch.nn.utils.rnn.pad_sequence(input_ids, batch_first=True, padding_value=self.tokenizer.pad_token_id)\n",
    "        labels = torch.nn.utils.rnn.pad_sequence(labels, batch_first=True, padding_value=-100)\n",
    "        attention_mask = (input_ids != self.tokenizer.pad_token_id).long()\n",
    "        return {\"input_ids\": input_ids, \"labels\": labels, \"attention_mask\": attention_mask}\n",
    "\n",
    "collator = DataCollatorForCausalPairs(tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f23635",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Load 4-bit base & apply LoRA ---\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16 if CFG[\"use_bf16\"] else torch.float16,\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(CFG[\"model_name\"], quantization_config=bnb_config, device_map=\"auto\", trust_remote_code=True)\n",
    "model = prepare_model_for_kbit_training(model, use_gradient_checkpointing=CFG[\"gradient_checkpointing\"])\n",
    "\n",
    "peft_cfg = LoraConfig(\n",
    "    r=CFG[\"lora_r\"],\n",
    "    lora_alpha=CFG[\"lora_alpha\"],\n",
    "    lora_dropout=CFG[\"lora_dropout\"],\n",
    "    target_modules=CFG[\"target_modules\"],\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "model = get_peft_model(model, peft_cfg)\n",
    "\n",
    "trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "total = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Trainable params: {trainable:,} / {total:,} ({100*trainable/total:.2f}%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5b12b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Trainer ---\n",
    "args = TrainingArguments(\n",
    "    output_dir=CFG[\"output_dir\"],\n",
    "    run_name=CFG[\"run_name\"],\n",
    "    num_train_epochs=CFG[\"epochs\"],\n",
    "    learning_rate=CFG[\"lr\"],\n",
    "    warmup_ratio=CFG[\"warmup_ratio\"],\n",
    "    per_device_train_batch_size=CFG[\"per_device_train_batch_size\"],\n",
    "    gradient_accumulation_steps=CFG[\"gradient_accumulation_steps\"],\n",
    "    logging_steps=CFG[\"logging_steps\"],\n",
    "    save_steps=CFG[\"save_steps\"],\n",
    "    evaluation_strategy=\"no\",  # set to \"steps\" if you add eval_dataset for ppl\n",
    "    bf16=CFG[\"use_bf16\"],\n",
    "    fp16=not CFG[\"use_bf16\"],\n",
    "    gradient_checkpointing=CFG[\"gradient_checkpointing\"],\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    report_to=[\"tensorboard\"],\n",
    "    optim=\"paged_adamw_8bit\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_ds,\n",
    "    data_collator=collator,\n",
    "    tokenizer=tokenizer,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)],\n",
    ")\n",
    "\n",
    "# To start training, uncomment:\n",
    "# trainer.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b68baa6",
   "metadata": {},
   "source": [
    "\n",
    "### Guardrails\n",
    "- Start with **CPT:ANCHOR = 0.9 : 0.1**. If instruction-following drops, go to **0.85 : 0.15** or **0.8 : 0.2**.\n",
    "- LR **1e-4** â†’ if unstable or hallucinations increase, drop to **7e-5**.\n",
    "- Limit to **1 epoch** first; only extend if perplexity & RAG metrics keep improving.\n",
    "- Ensure **dedup & PII scrub** on your corpus.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8739e5ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Simple Perplexity Eval (dev texts) ---\n",
    "import numpy as np\n",
    "\n",
    "@torch.no_grad()\n",
    "def perplexity_on_texts(model, tokenizer, texts: List[str], max_length: int = 1024) -> float:\n",
    "    model.eval()\n",
    "    nlls = []\n",
    "    for t in texts:\n",
    "        enc = tokenizer(t, return_tensors=\"pt\", truncation=True, max_length=max_length).to(model.device)\n",
    "        labels = enc[\"input_ids\"].clone()\n",
    "        out = model(**enc, labels=labels)\n",
    "        nlls.append(out.loss.detach().float().item())\n",
    "    ppl = math.exp(float(np.mean(nlls)))\n",
    "    return ppl\n",
    "\n",
    "# Example (replace with held-out paragraphs):\n",
    "# dev_texts = [\"Held-out paragraph ...\", \"Another paragraph ...\"]\n",
    "# print(\"Perplexity:\", perplexity_on_texts(model, tokenizer, dev_texts))\n",
    "\n",
    "def run_rag_eval_stub(checkpoint_dir: str):\n",
    "    # Integrate your  RAG eval harness here.\n",
    "    return {\"faithfulness\": None, \"completeness\": None, \"specificity\": None, \"conciseness\": None, \"schema_pass_rate\": None}\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sftenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
