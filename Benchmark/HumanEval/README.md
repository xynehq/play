# HumanEval: Hand-Written Evaluation Set 

This is an evaluation harness for the HumanEval problem solving dataset described in the paper "[Evaluating Large Language Models Trained on Code](https://arxiv.org/abs/2107.03374)".

This repository has been extended to support **Rust benchmarking** with integrated evaluation capabilities for testing language models on Rust code generation tasks.

---

## Table of Contents

- [What is HumanEval?](#what-is-humaneval)
- [How It Works](#how-it-works)
- [Quick Start](#quick-start)
- [Installation](#installation)
- [Rust Benchmarking Setup](#rust-benchmarking-setup)
- [Running Benchmarks](#running-benchmarks)
- [Extending with Multiple Datasets](#extending-with-multiple-datasets)
- [Usage](#usage)
- [Known Issues](#known-issues)
- [Citation](#citation)

---

## What is HumanEval?

**HumanEval** is a benchmark dataset designed to evaluate the functional correctness of code generated by large language models (LLMs). It consists of 164 hand-written programming problems, each including:

- **Function signature**: The expected function name and parameters
- **Docstring**: A description of what the function should do
- **Test cases**: Multiple unit tests to verify correctness

The benchmark measures how well models can generate code that passes all test cases, providing metrics like:
- **pass@1**: Percentage of problems solved with a single attempt
- **pass@k**: Percentage of problems solved with k attempts (e.g., pass@10, pass@100)

---

## How It Works

### Evaluation Process

1. **Problem Loading**: The evaluator loads programming problems from a dataset (e.g., `HumanEval.jsonl.gz` for Python or `humaneval-rust.jsonl.gz` for Rust)

2. **Code Generation**: Your language model generates code completions for each problem
   - The model receives the function signature and docstring as a prompt
   - It generates the function body to solve the problem

3. **Execution & Testing**: Generated code is executed against test cases
   - Code is compiled (for Rust) or run (for Python)
   - Multiple test cases validate correctness
   - Results are categorized as: `passed`, `failed`, or `timed out`

4. **Metrics Calculation**: Pass@k metrics are computed
   - **pass@1**: Success rate with 1 sample per problem
   - **pass@k**: Success rate with k samples per problem (using unbiased estimator)

### Rust Evaluation

This repository includes integrated Rust evaluation capabilities:
- Generates Rust code completions from prompts
- Compiles code using `cargo` 
- Runs comprehensive test suites
- Handles compilation errors and runtime failures
- Calculates pass@k metrics for Rust benchmarks

---

## Quick Start

### **Option 1: Automated Setup with setup_humaneval.py** âš¡ (Recommended)

The easiest way to get started is using the automated setup script:

```bash
python3 setup_humaneval.py
```

**This interactive script will:**
1. âœ… Create Python 3.7+ virtual environment
2. âœ… Install all dependencies (numpy, requests, tqdm, etc.)
3. âœ… Clone HumanEval repository (if needed)
4. âœ… Copy Rust dataset to proper location
5. âœ… Prompt for API configuration (key, base URL, models)
6. âœ… Choose benchmark language (Rust/Python)
7. âœ… **Automatically run the benchmark** via `run_humaneval_api.py`
8. âœ… Save results to `result/` directory

**What you need before running:**
- Python 3.7+ installed
- Rust toolchain (cargo) for Rust benchmarking
- API credentials ready

**Run it:**
```bash
python3 setup_humaneval.py
```

The script will guide you through the setup interactively!

---

### **Option 2: Run Benchmark Only with run_humaneval_api.py**

If your environment is already set up, use the benchmark script directly:

```bash
python3 run_humaneval_api.py
```

**What it does:**
- Loads problems from `data/humaneval-rust.jsonl.gz` (or Python dataset)
- Generates code completions via API calls
- Evaluates correctness by compiling and testing code
- Calculates pass@1 and pass@10 metrics
- Saves detailed results and logs

**Prerequisites:**
- Python environment with dependencies installed
- Rust dataset at `data/humaneval-rust.jsonl.gz` (for Rust mode)
- API credentials configured in the script
- Rust toolchain installed (for Rust benchmarking)

**Configuration in script:**
```python
# Edit these in run_humaneval_api.py:
API_KEY = 'your-api-key'
BASE_URL = 'https://your-api-endpoint.com'
FINE_TUNED_MODEL = "your-model-name"
BASE_MODEL = "base-model-name"
LANGUAGE = "rust"  # or "python"
```

---

### **Option 3: Manual Step-by-Step**

If you prefer complete manual control:

```bash
# 1. Create virtual environment
python3 -m venv venv_humaneval
source venv_humaneval/bin/activate

# 2. Install dependencies
pip install numpy requests tqdm python-dotenv

# 3. Install human_eval package
git clone https://github.com/openai/human-eval
pip install -e human-eval

# 4. Get Rust dataset (see "Obtaining the Rust Dataset" section)
mkdir -p data
# Download humaneval-rust.jsonl.gz to data/

# 5. Configure run_humaneval_api.py with your settings

# 6. Run benchmark
python3 run_humaneval_api.py
```

---

## Installation

### Prerequisites

- **Python 3.7+**
- **Rust toolchain** (for Rust benchmarking)
- **API keys** (Anthropic, OpenAI, etc.)

### Option 1: Automated One-Command Setup + Benchmark (Recommended) âš¡

Use the Python automation script:

```bash
# Prerequisite: Create .env file with API keys
cp .env.example .env
# Edit .env with your actual API keys

# One command does EVERYTHING:
python3 setup_and_run.py
```

**What this does automatically:**
- âœ… Install Rust toolchain (rustc, cargo)
- âœ… Install Python 3 dependencies and pip
- âœ… Install the human_eval package
- âœ… Install system utilities (Homebrew/apt packages)
- âœ… Verify installation with test compilation
- âœ… Configure environment variables
- âœ… Create configuration templates
- âœ… **Run the complete benchmark** (calls run_humaneval_api.py)
- âœ… Save results to result/ directory

**Advanced: Clone repo and automate:**
```bash
python3 setup_and_run.py --clone-repo https://github.com/openai/human-eval.git --clone-dir my-benchmark
```

### Option 2: Bash Setup Script (Setup Only)

Use bash script for setup only (then run benchmark manually):

```bash
chmod +x setup_rust_benchmark.sh
./setup_rust_benchmark.sh

# Then run benchmark separately:
python3 run_humaneval_api.py
```

### Option 2: Manual Setup

**1. Install Python environment:**
```bash
conda create -n codex python=3.7
conda activate codex
```

**2. Install this repository:**
```bash
git clone https://github.com/openai/human-eval
cd human-eval
pip install -e .
```

**3. Install Rust (for Rust benchmarking):**
```bash
curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh
source $HOME/.cargo/env
```

**4. Install additional dependencies:**
```bash
pip install numpy openai tqdm anthropic
```

---

## Rust Benchmarking Setup

### Obtaining the Rust Dataset

The Rust benchmarking requires the HumanEval Rust dataset from the MultiPL-E project. Here are several ways to obtain it:

#### **Method 1: Clone MultiPL-E Repository (Recommended)**

```bash
# Clone the MultiPL-E repository
git clone https://github.com/nuprl/MultiPL-E.git

# Create data directory if it doesn't exist
mkdir -p data

# Copy the Rust dataset
cp MultiPL-E/datasets/humaneval-rs.jsonl.gz data/humaneval-rust.jsonl.gz

# Verify the file
ls -lh data/humaneval-rust.jsonl.gz
```

#### **Method 2: Direct Download**

```bash
# Create data directory
mkdir -p data

# Download directly from GitHub
wget https://github.com/nuprl/MultiPL-E/raw/main/datasets/humaneval-rs.jsonl.gz \
  -O data/humaneval-rust.jsonl.gz

# Or using curl
curl -L https://github.com/nuprl/MultiPL-E/raw/main/datasets/humaneval-rs.jsonl.gz \
  -o data/humaneval-rust.jsonl.gz

# Verify the file
ls -lh data/humaneval-rust.jsonl.gz
```

#### **Method 3: Automatic via setup_humaneval.py**

The `setup_humaneval.py` script can automatically handle dataset placement:

```bash
# Run setup script
python3 setup_humaneval.py

# It will:
# 1. Clone HumanEval repo if needed
# 2. Copy Rust dataset from cloned repo to data/
# 3. Verify dataset exists before running benchmark
```

### Dataset Location

**Required location:** `data/humaneval-rust.jsonl.gz`

The dataset must be placed in a `data/` folder in your working directory. Both `run_humaneval_api.py` and `setup_humaneval.py` expect to find it at this exact path.

### Configuration

Create a `.env` file with your API keys:
```bash
cp .env.example .env
```

Edit `.env` and add your keys:
```env
ANTHROPIC_API_KEY=your_anthropic_api_key_here
OPENAI_API_KEY=your_openai_api_key_here

MODEL_NAME=claude-3-7-sonnet-20250219
BASE_MODEL_NAME=claude-3-7-sonnet-20250219

NUM_SAMPLES=10
TEMPERATURE=0.8
MAX_TOKENS=2048
TIMEOUT=60
```

### Verify Installation

```bash
# Check Rust installation
cargo --version
rustc --version

# Check Python environment
python3 -c "from human_eval.evaluation import evaluate_rust_correctness; print('âœ… Setup successful')"
```

---

## Running Benchmarks

### Automated Benchmark (Recommended) âš¡

**One command does everything:**

```bash
python3 setup_and_run.py
```

This single command will:
1. Setup complete environment (if needed)
2. Load Rust problems from `data/humaneval-rust.jsonl.gz`
3. Generate completions using your configured models (fine-tuned + base)
4. Evaluate correctness using Rust compilation and testing
5. Calculate pass@1 and pass@10 metrics
6. Save detailed results to `result/` directory

### Manual Benchmark Run

If environment is already set up, run benchmark only:

```bash
python3 run_humaneval_api.py
```

**Output files** (using model names):
- `result/{MODEL_NAME}_rust.jsonl` - Model completions (e.g., kat-dev-hs-72b_rust.jsonl)
- `result/{MODEL_NAME}_rust.jsonl_results.jsonl` - Evaluation results
- `result/{MODEL_NAME}_vs_{BASE_MODEL}_rust_results.json` - Comparison summary

### Python Benchmark (Original)

For Python evaluation, follow the original HumanEval workflow:

```bash
# 1. Generate samples (implement generate_one_completion)
from human_eval.data import write_jsonl, read_problems

problems = read_problems()
num_samples_per_task = 200
samples = [
    dict(task_id=task_id, completion=generate_one_completion(problems[task_id]["prompt"]))
    for task_id in problems
    for _ in range(num_samples_per_task)
]
write_jsonl("samples.jsonl", samples)

# 2. Evaluate the samples
evaluate_functional_correctness samples.jsonl
```

---

## Extending with Multiple Datasets

### Using MultiPL-E for Rust

The [MultiPL-E](https://github.com/nuprl/MultiPL-E) repository provides HumanEval translations for multiple programming languages, including Rust.

**To use MultiPL-E Rust dataset:**

1. **Clone the MultiPL-E repository:**
   ```bash
   git clone https://github.com/nuprl/MultiPL-E.git
   ```

2. **Copy the Rust dataset:**
   ```bash
   # The Rust prompts are in the MultiPL-E repository
   cp MultiPL-E/datasets/humaneval-rs.jsonl.gz data/humaneval-rust.jsonl.gz
   ```

3. **Or download directly:**
   ```bash
   # You can also download specific language datasets from MultiPL-E releases
   wget https://github.com/nuprl/MultiPL-E/raw/main/datasets/humaneval-rs.jsonl.gz -O data/humaneval-rust.jsonl.gz
   ```

### Adding Other Languages

To extend this benchmarking framework to other languages:

1. **Get the dataset** from MultiPL-E or create your own translation
   ```bash
   # Examples for other languages:
   # JavaScript: humaneval-js.jsonl.gz
   # TypeScript: humaneval-ts.jsonl.gz  
   # C++: humaneval-cpp.jsonl.gz
   # Go: humaneval-go.jsonl.gz
   ```

2. **Add language-specific evaluation** in `human_eval/evaluation.py`:
   ```python
   def evaluate_<language>_correctness(
       sample_file,
       k=[1, 10, 100],
       n_workers=4,
       timeout=3.0
   ):
       # Implement language-specific compilation and testing
       pass
   ```

3. **Update the benchmarking script** (`run_humaneval_api.py`):
   ```python
   LANGUAGE = "your_language"  # Set language
   PROBLEMS_FILE = "data/humaneval-yourlang.jsonl.gz"
   ```

### Available MultiPL-E Languages

MultiPL-E provides HumanEval translations for 18+ languages:
- **Compiled**: Rust, C++, C#, Java, Go, Swift, Scala, D
- **Interpreted**: Python, JavaScript, TypeScript, Ruby, PHP, Lua, Perl, R
- **Functional**: Haskell, OCaml, F#, Racket, Elixir, Julia
- **Other**: Bash, MATLAB

---

## Usage

### API Configuration

Customize model settings in `run_humaneval_api.py`:

```python
# Model Configuration
MODEL_NAME = "your-finetuned-model"  # Fine-tuned model
BASE_MODEL_NAME = "base-model"       # Base model for comparison

# Benchmark Settings
NUM_SAMPLES = 10        # Samples per problem
TEMPERATURE = 0.8       # Sampling temperature
MAX_TOKENS = 2048       # Max completion tokens
TIMEOUT = 60            # Execution timeout (seconds)

# Language Selection
LANGUAGE = "rust"       # or "python"
```

### Evaluation Options

For Python evaluation with custom options:
```bash
# Evaluate with specific k values
evaluate_functional_correctness samples.jsonl --k=1,10,100

# Use custom problem file
evaluate_functional_correctness samples.jsonl --problem_file=data/custom_problems.jsonl

# Adjust workers and timeout
evaluate_functional_correctness samples.jsonl --n_workers=8 --timeout=5.0

# See all options
evaluate_functional_correctness --help
```

---

## Known Issues

### Memory Issues

While evaluation uses very little memory, you might see the following error when the system is running out of RAM:
```
malloc: can't allocate region
```
**Solution**: Free some memory and try again, as this may cause correct programs to fail.

### Rust Compilation Timeouts

Long-running Rust compilation may timeout. Adjust the timeout in configuration:
```python
TIMEOUT = 120  # Increase timeout for complex Rust code
```

### API Rate Limits

When benchmarking with API models, you may hit rate limits:
- **Solution**: Reduce `NUM_SAMPLES` or add delays between requests
- Monitor your API usage and adjust accordingly

---

## Results Interpretation

### Benchmark Metrics

After running benchmarks, you'll see results like:

```
ðŸ“Š Results for your-model:
  pass@1: 0.1628 (16.28%)
  pass@10: 0.4103 (41.03%)
```

**Interpretation:**
- **pass@1 = 16.28%**: Model solves 16.28% of problems on first try
- **pass@10 = 41.03%**: Model solves 41.03% of problems within 10 attempts

### Comparison Analysis

The benchmark also provides improvement analysis:
```
ðŸ“ˆ IMPROVEMENT ANALYSIS
pass@1:
  Fine-tuned: 0.1628 (16.28%)
  Base:       0.0699 (6.99%)
  Î” Absolute: +9.29 percentage points
  Î” Relative: +133.03% improvement
```

---

## Script Documentation

### setup_humaneval.py

**Purpose:** Automated one-command setup and benchmark execution

**What it does:**
1. Detects Python 3.7+ installation
2. Creates isolated virtual environment (`venv_humaneval/`)
3. Installs all required dependencies (numpy, requests, tqdm, etc.)
4. Clones HumanEval repository if not present
5. Copies Rust dataset to correct location
6. Interactively prompts for:
   - API Key
   - Base URL
   - Fine-tuned model name
   - Base model name (optional)
   - Benchmark language (Rust/Python)
7. Automatically runs `run_humaneval_api.py`
8. Saves results to `result/` directory

**Usage:**
```bash
python3 setup_humaneval.py
```

**Interactive prompts:**
```
Configure API settings now? (y/n): y
API Key: your-api-key-here
Base URL: https://grid.ai.juspay.net
Fine-tuned Model Name: kat-dev-hs-72b
Base Model Name: kat-dev-base-72b
Choose Benchmark Language: 1 (for Rust)
```

**Requirements:**
- Python 3.7+ installed on system
- Rust toolchain (cargo) for Rust benchmarking
- Internet connection for cloning repo

---

### run_humaneval_api.py

**Purpose:** Execute HumanEval benchmark via API calls

**What it does:**
1. Loads problems from dataset (`data/humaneval-rust.jsonl.gz`)
2. Makes API calls to generate code completions
3. For each problem, generates N samples (default: 10)
4. Evaluates completions by:
   - Compiling Rust code with cargo
   - Running test suites
   - Capturing pass/fail results
5. Calculates pass@k metrics (pass@1, pass@10)
6. Saves results and generates comparison reports
7. Provides real-time logging during execution

**Configuration:**
Edit the script to set:
```python
# API Configuration
API_KEY = 'your-api-key'
BASE_URL = 'https://grid.ai.juspay.net'

# Models
FINE_TUNED_MODEL = "kat-dev-hs-72b"
BASE_MODEL = "kat-dev-base-72b"

# Settings
LANGUAGE = "rust"  # or "python"
```

**Usage:**
```bash
python3 run_humaneval_api.py
```

**Output files:**
- `result/{MODEL}_rust.jsonl` - Generated completions
- `result/{MODEL}_rust.jsonl_results.jsonl` - Detailed results
- `result/{MODEL}_vs_{BASE}_rust_results.json` - Comparison metrics
- `result/logs/{MODEL}_rust_task_log.txt` - Execution log
- `result/logs/{MODEL}_rust_realtime_log.jsonl` - Structured log

**Features:**
- Real-time progress tracking with tqdm
- Parallel test execution (configurable workers)
- Timeout handling for long-running tests
- Automatic result comparison between models
- Comprehensive error handling

---

## File Structure

```
working-directory/
â”œâ”€â”€ setup_humaneval.py          # ðŸš€ Automated setup + benchmark script
â”œâ”€â”€ run_humaneval_api.py        # ðŸ”§ Benchmark execution script
â”œâ”€â”€ venv_humaneval/             # Virtual environment (created by setup_humaneval.py)
â”‚   â”œâ”€â”€ bin/
â”‚   â”‚   â”œâ”€â”€ python
â”‚   â”‚   â””â”€â”€ pip
â”‚   â””â”€â”€ lib/
â”œâ”€â”€ data/                       # Dataset directory
â”‚   â”œâ”€â”€ HumanEval.jsonl.gz     # Python problems (from cloned repo)
â”‚   â””â”€â”€ humaneval-rust.jsonl.gz # Rust problems (required!)
â”œâ”€â”€ human-eval/                 # Cloned HumanEval repository
â”‚   â”œâ”€â”€ setup.py
â”‚   â”œâ”€â”€ README.md
â”‚   â”œâ”€â”€ human_eval/            # Core evaluation package
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ data.py           # Dataset utilities
â”‚   â”‚   â”œâ”€â”€ evaluation.py     # Evaluation functions
â”‚   â”‚   â”œâ”€â”€ execution.py      # Code execution
â”‚   â”‚   â””â”€â”€ evaluate_functional_correctness.py
â”‚   â””â”€â”€ data/
â”‚       â””â”€â”€ HumanEval.jsonl.gz
â””â”€â”€ result/                    # Results (auto-created)
    â”œâ”€â”€ kat-dev-hs-72b_rust.jsonl
    â”œâ”€â”€ kat-dev-hs-72b_rust.jsonl_results.jsonl
    â”œâ”€â”€ kat-dev-hs-72b_vs_kat-dev-base-72b_rust_results.json
    â””â”€â”€ logs/
        â”œâ”€â”€ kat-dev-hs-72b_rust_task_log.txt
        â””â”€â”€ kat-dev-hs-72b_rust_realtime_log.jsonl
```

**Key directories:**
- `venv_humaneval/` - Isolated Python environment
- `data/` - Dataset storage (Rust dataset goes here!)
- `human-eval/` - Cloned repository with evaluation code
- `result/` - All benchmark results and logs


---

## Support & Resources

- **Original HumanEval**: [github.com/openai/human-eval](https://github.com/openai/human-eval)
- **MultiPL-E (Multi-language)**: [github.com/nuprl/MultiPL-E](https://github.com/nuprl/MultiPL-E)
- **Paper**: [Evaluating Large Language Models Trained on Code](https://arxiv.org/abs/2107.03374)
- **Setup Guide**: See `SETUP_GUIDE.md` for detailed installation instructions


For issues or questions:
- Check logs in `result/*_progress.log`
- Verify dataset: `ls -lh data/humaneval-rust.jsonl.gz`
- Test setup: `python3 -c "from human_eval.evaluation import evaluate_rust_correctness; print('OK')"`
