# HumanEval: Hand-Written Evaluation Set 

This is an evaluation harness for the HumanEval problem solving dataset described in the paper "[Evaluating Large Language Models Trained on Code](https://arxiv.org/abs/2107.03374)".

This repository has been extended to support **Rust benchmarking** with integrated evaluation capabilities for testing language models on Rust code generation tasks.

---

## Table of Contents

- [What is HumanEval?](#what-is-humaneval)
- [How It Works](#how-it-works)
- [Quick Start](#quick-start)
- [Installation](#installation)
- [Rust Benchmarking Setup](#rust-benchmarking-setup)
- [Running Benchmarks](#running-benchmarks)
- [Extending with Multiple Datasets](#extending-with-multiple-datasets)
- [Usage](#usage)
- [Known Issues](#known-issues)
- [Citation](#citation)

---

## What is HumanEval?

**HumanEval** is a benchmark dataset designed to evaluate the functional correctness of code generated by large language models (LLMs). It consists of 164 hand-written programming problems, each including:

- **Function signature**: The expected function name and parameters
- **Docstring**: A description of what the function should do
- **Test cases**: Multiple unit tests to verify correctness

The benchmark measures how well models can generate code that passes all test cases, providing metrics like:
- **pass@1**: Percentage of problems solved with a single attempt
- **pass@k**: Percentage of problems solved with k attempts (e.g., pass@10, pass@100)

---

## How It Works

### Evaluation Process

1. **Problem Loading**: The evaluator loads programming problems from a dataset (e.g., `HumanEval.jsonl.gz` for Python or `humaneval-rust.jsonl.gz` for Rust)

2. **Code Generation**: Your language model generates code completions for each problem
   - The model receives the function signature and docstring as a prompt
   - It generates the function body to solve the problem

3. **Execution & Testing**: Generated code is executed against test cases
   - Code is compiled (for Rust) or run (for Python)
   - Multiple test cases validate correctness
   - Results are categorized as: `passed`, `failed`, or `timed out`

4. **Metrics Calculation**: Pass@k metrics are computed
   - **pass@1**: Success rate with 1 sample per problem
   - **pass@k**: Success rate with k samples per problem (using unbiased estimator)

### Rust Evaluation

This repository includes integrated Rust evaluation capabilities:
- Generates Rust code completions from prompts
- Compiles code using `cargo` 
- Runs comprehensive test suites
- Handles compilation errors and runtime failures
- Calculates pass@k metrics for Rust benchmarks

---

## Quick Start

**Option 1: All-in-One Python Script (Recommended)**

Run everything with a single command:

```bash
# This will: install all dependencies, setup environment, AND run the benchmark
python3 setup_and_run.py
```

Before running, create a `.env` file with your API keys:
```bash
cp .env.example .env
# Edit .env with your actual API keys (ANTHROPIC_API_KEY, etc.)
```

**Option 2: Step-by-Step with Bash Script**

```bash
# 1. Run the automated setup script (installs Rust, Python deps, etc.)
chmod +x setup_rust_benchmark.sh
./setup_rust_benchmark.sh

# 2. Configure your API keys
cp .env.example .env
# Edit .env with your actual API keys

# 3. Run the Rust benchmark
python3 run_humaneval_api.py
```

That's it! Results will be saved in the `result/` directory.

---

## Installation

### Prerequisites

- **Python 3.7+**
- **Rust toolchain** (for Rust benchmarking)
- **API keys** (Anthropic, OpenAI, etc.)

### Option 1: All-in-One Python Script (Recommended)

Use the Python setup script for complete automation:

```bash
python3 setup_and_run.py
```

This single command will:
- âœ… Install Rust toolchain (rustc, cargo)
- âœ… Install Python 3 dependencies and pip
- âœ… Install the human_eval package
- âœ… Install system utilities (Homebrew/apt packages)
- âœ… Verify installation with test compilation
- âœ… Configure environment variables
- âœ… Create configuration templates
- âœ… **Run the complete benchmark** (if .env file exists)

### Option 2: Bash Setup Script

Use the bash script for setup only (benchmark run separately):

```bash
chmod +x setup_rust_benchmark.sh
./setup_rust_benchmark.sh
```

Then run the benchmark:
```bash
python3 run_humaneval_api.py
```

### Option 2: Manual Setup

**1. Install Python environment:**
```bash
conda create -n codex python=3.7
conda activate codex
```

**2. Install this repository:**
```bash
git clone https://github.com/openai/human-eval
cd human-eval
pip install -e .
```

**3. Install Rust (for Rust benchmarking):**
```bash
curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh
source $HOME/.cargo/env
```

**4. Install additional dependencies:**
```bash
pip install numpy openai tqdm anthropic
```

---

## Rust Benchmarking Setup

### Dataset Requirements

The Rust benchmarking requires the HumanEval Rust dataset. Place it at:
```
data/humaneval-rust.jsonl.gz
```

### Configuration

Create a `.env` file with your API keys:
```bash
cp .env.example .env
```

Edit `.env` and add your keys:
```env
ANTHROPIC_API_KEY=your_anthropic_api_key_here
OPENAI_API_KEY=your_openai_api_key_here

MODEL_NAME=claude-3-7-sonnet-20250219
BASE_MODEL_NAME=claude-3-7-sonnet-20250219

NUM_SAMPLES=10
TEMPERATURE=0.8
MAX_TOKENS=2048
TIMEOUT=60
```

### Verify Installation

```bash
# Check Rust installation
cargo --version
rustc --version

# Check Python environment
python3 -c "from human_eval.evaluation import evaluate_rust_correctness; print('âœ… Setup successful')"
```

---

## Running Benchmarks

### Rust Benchmark (API-based)

Run the complete Rust benchmark using API models:

```bash
python3 run_humaneval_api.py
```

This will:
1. Load Rust problems from `data/humaneval-rust.jsonl.gz`
2. Generate completions using your configured models (fine-tuned + base)
3. Evaluate correctness using Rust compilation and testing
4. Calculate pass@1 and pass@10 metrics
5. Save detailed results to `result/` directory

**Output files:**
- `result/api_finetuned_rust.jsonl` - Fine-tuned model completions
- `result/api_base_rust.jsonl` - Base model completions  
- `result/api_*_results.jsonl` - Evaluation results with pass/fail status
- `result/api_rust_benchmark_results.json` - Summary metrics and comparison

### Python Benchmark (Original)

For Python evaluation, follow the original HumanEval workflow:

```bash
# 1. Generate samples (implement generate_one_completion)
from human_eval.data import write_jsonl, read_problems

problems = read_problems()
num_samples_per_task = 200
samples = [
    dict(task_id=task_id, completion=generate_one_completion(problems[task_id]["prompt"]))
    for task_id in problems
    for _ in range(num_samples_per_task)
]
write_jsonl("samples.jsonl", samples)

# 2. Evaluate the samples
evaluate_functional_correctness samples.jsonl
```

---

## Extending with Multiple Datasets

### Using MultiPL-E for Rust

The [MultiPL-E](https://github.com/nuprl/MultiPL-E) repository provides HumanEval translations for multiple programming languages, including Rust.

**To use MultiPL-E Rust dataset:**

1. **Clone the MultiPL-E repository:**
   ```bash
   git clone https://github.com/nuprl/MultiPL-E.git
   ```

2. **Copy the Rust dataset:**
   ```bash
   # The Rust prompts are in the MultiPL-E repository
   cp MultiPL-E/datasets/humaneval-rs.jsonl.gz data/humaneval-rust.jsonl.gz
   ```

3. **Or download directly:**
   ```bash
   # You can also download specific language datasets from MultiPL-E releases
   wget https://github.com/nuprl/MultiPL-E/raw/main/datasets/humaneval-rs.jsonl.gz -O data/humaneval-rust.jsonl.gz
   ```

### Adding Other Languages

To extend this benchmarking framework to other languages:

1. **Get the dataset** from MultiPL-E or create your own translation
   ```bash
   # Examples for other languages:
   # JavaScript: humaneval-js.jsonl.gz
   # TypeScript: humaneval-ts.jsonl.gz  
   # C++: humaneval-cpp.jsonl.gz
   # Go: humaneval-go.jsonl.gz
   ```

2. **Add language-specific evaluation** in `human_eval/evaluation.py`:
   ```python
   def evaluate_<language>_correctness(
       sample_file,
       k=[1, 10, 100],
       n_workers=4,
       timeout=3.0
   ):
       # Implement language-specific compilation and testing
       pass
   ```

3. **Update the benchmarking script** (`run_humaneval_api.py`):
   ```python
   LANGUAGE = "your_language"  # Set language
   PROBLEMS_FILE = "data/humaneval-yourlang.jsonl.gz"
   ```

### Available MultiPL-E Languages

MultiPL-E provides HumanEval translations for 18+ languages:
- **Compiled**: Rust, C++, C#, Java, Go, Swift, Scala, D
- **Interpreted**: Python, JavaScript, TypeScript, Ruby, PHP, Lua, Perl, R
- **Functional**: Haskell, OCaml, F#, Racket, Elixir, Julia
- **Other**: Bash, MATLAB

---

## Usage

### API Configuration

Customize model settings in `run_humaneval_api.py`:

```python
# Model Configuration
MODEL_NAME = "your-finetuned-model"  # Fine-tuned model
BASE_MODEL_NAME = "base-model"       # Base model for comparison

# Benchmark Settings
NUM_SAMPLES = 10        # Samples per problem
TEMPERATURE = 0.8       # Sampling temperature
MAX_TOKENS = 2048       # Max completion tokens
TIMEOUT = 60            # Execution timeout (seconds)

# Language Selection
LANGUAGE = "rust"       # or "python"
```

### Evaluation Options

For Python evaluation with custom options:
```bash
# Evaluate with specific k values
evaluate_functional_correctness samples.jsonl --k=1,10,100

# Use custom problem file
evaluate_functional_correctness samples.jsonl --problem_file=data/custom_problems.jsonl

# Adjust workers and timeout
evaluate_functional_correctness samples.jsonl --n_workers=8 --timeout=5.0

# See all options
evaluate_functional_correctness --help
```

---

## Known Issues

### Memory Issues

While evaluation uses very little memory, you might see the following error when the system is running out of RAM:
```
malloc: can't allocate region
```
**Solution**: Free some memory and try again, as this may cause correct programs to fail.

### Rust Compilation Timeouts

Long-running Rust compilation may timeout. Adjust the timeout in configuration:
```python
TIMEOUT = 120  # Increase timeout for complex Rust code
```

### API Rate Limits

When benchmarking with API models, you may hit rate limits:
- **Solution**: Reduce `NUM_SAMPLES` or add delays between requests
- Monitor your API usage and adjust accordingly

---

## Results Interpretation

### Benchmark Metrics

After running benchmarks, you'll see results like:

```
ðŸ“Š Results for your-model:
  pass@1: 0.1628 (16.28%)
  pass@10: 0.4103 (41.03%)
```

**Interpretation:**
- **pass@1 = 16.28%**: Model solves 16.28% of problems on first try
- **pass@10 = 41.03%**: Model solves 41.03% of problems within 10 attempts

### Comparison Analysis

The benchmark also provides improvement analysis:
```
ðŸ“ˆ IMPROVEMENT ANALYSIS
pass@1:
  Fine-tuned: 0.1628 (16.28%)
  Base:       0.0699 (6.99%)
  Î” Absolute: +9.29 percentage points
  Î” Relative: +133.03% improvement
```

---

## File Structure

```
human-eval/
â”œâ”€â”€ setup_and_run.py            # Python: Complete setup + benchmark runner (ONE COMMAND)
â”œâ”€â”€ setup_rust_benchmark.sh     # Bash: Automated setup script (setup only)
â”œâ”€â”€ run_humaneval_api.py        # Main Rust benchmarking script
â”œâ”€â”€ .env.example                # API configuration template
â”œâ”€â”€ .env                        # Your API keys (you create this)
â”œâ”€â”€ SETUP_GUIDE.md              # Detailed setup instructions
â”œâ”€â”€ README.md                   # This file
â”œâ”€â”€ human_eval/                 # Core evaluation package
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ data.py                # Dataset utilities
â”‚   â”œâ”€â”€ evaluation.py          # Rust evaluation functions
â”‚   â”œâ”€â”€ execution.py           # Code execution utilities
â”‚   â””â”€â”€ evaluate_functional_correctness.py
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ HumanEval.jsonl.gz     # Python problems
â”‚   â””â”€â”€ humaneval-rust.jsonl.gz # Rust problems
â””â”€â”€ result/                     # Benchmark results (auto-created)
    â”œâ”€â”€ api_finetuned_rust.jsonl
    â”œâ”€â”€ api_base_rust.jsonl
    â””â”€â”€ api_rust_benchmark_results.json
```

---

## Support & Resources

- **Original HumanEval**: [github.com/openai/human-eval](https://github.com/openai/human-eval)
- **MultiPL-E (Multi-language)**: [github.com/nuprl/MultiPL-E](https://github.com/nuprl/MultiPL-E)
- **Paper**: [Evaluating Large Language Models Trained on Code](https://arxiv.org/abs/2107.03374)
- **Setup Guide**: See `SETUP_GUIDE.md` for detailed installation instructions


For issues or questions:
- Check logs in `result/*_progress.log`
- Verify dataset: `ls -lh data/humaneval-rust.jsonl.gz`
- Test setup: `python3 -c "from human_eval.evaluation import evaluate_rust_correctness; print('OK')"`
