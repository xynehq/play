#!/usr/bin/env python3
"""
polyglot

One-step Aider Polyglot Benchmark setup + evaluation script.

‚úÖ Features:
- Checks Docker installation and status
- Sets up Aider repository and polyglot benchmark exercises
- Builds Docker container for safe evaluation
- Runs benchmark with configurable models and settings
- Generates comprehensive reports
- Handles cleanup and error recovery
"""

import os
import platform
import subprocess
import sys
import json
import datetime
from pathlib import Path
import shutil

# -----------------------------------------------------------------------------
# PREREQUISITES CHECK - Run before anything else
# -----------------------------------------------------------------------------
def check_prerequisites():
    """
    Check Docker installation and running status.
    All other requirements are handled automatically by the script.
    """
    print("="*80)
    print("üîç CHECKING DOCKER (Required for Safe Evaluation)")
    print("="*80)
    
    system = platform.system()
    docker_installed = False
    docker_running = False
    
    print(f"\nüìç Operating System: {system} {platform.release()}")
    
    # Check Docker Installation and Status
    print("\nüê≥ Docker Check:")
    
    try:
        # Check if docker command exists
        result = subprocess.run(
            ["docker", "--version"],
            capture_output=True,
            text=True,
            check=True
        )
        docker_version = result.stdout.strip()
        print(f"   ‚úÖ Docker installed: {docker_version}")
        docker_installed = True
        
        # Check if Docker daemon is running
        try:
            subprocess.run(
                ["docker", "info"],
                stdout=subprocess.DEVNULL,
                stderr=subprocess.DEVNULL,
                check=True,
                timeout=5
            )
            print("   ‚úÖ Docker daemon is running")
            docker_running = True
        except (subprocess.CalledProcessError, subprocess.TimeoutExpired):
            print("   ‚ö†Ô∏è  Docker is installed but NOT running")
            
    except FileNotFoundError:
        print("   ‚ùå Docker is NOT installed")
    except subprocess.CalledProcessError:
        print("   ‚ùå Docker command failed")
    
    # Print Summary
    print("\n" + "="*80)
    
    if docker_installed and docker_running:
        print("‚úÖ DOCKER CHECK PASSED")
        print("="*80)
        print("üöÄ Proceeding with setup...")
        print("="*80)
        return True
    
    # Docker not installed or not running
    if not docker_installed:
        print("‚ùå DOCKER NOT INSTALLED")
        print("="*80)
        print("\nüê≥ DOCKER INSTALLATION REQUIRED")
        print("="*80)
        
        if system == "Darwin":
            print("\nüì¶ macOS Installation Options:\n")
            print("Option 1: OrbStack (Recommended - Lightweight)")
            print("   1. Visit: https://orbstack.dev")
            print("   2. Download and install")
            print("   3. Open OrbStack from Applications")
            print("   4. Verify: docker info\n")
            
            print("Option 2: Docker Desktop")
            print("   1. Visit: https://www.docker.com/products/docker-desktop")
            print("   2. Download Docker Desktop for Mac")
            if platform.machine() == "arm64":
                print("      ‚Üí Choose 'Apple Silicon' version")
            else:
                print("      ‚Üí Choose 'Intel Chip' version")
            print("   3. Install and launch Docker Desktop")
            print("   4. Verify: docker info\n")
            
            print("Option 3: Homebrew")
            print("   brew install --cask orbstack")
            print("   # OR")
            print("   brew install --cask docker\n")
            
        elif system == "Linux":
            print("\nüì¶ Linux Installation:\n")
            try:
                with open("/etc/os-release") as f:
                    os_info = f.read().lower()
                
                if "ubuntu" in os_info or "debian" in os_info:
                    print("Ubuntu/Debian:")
                    print("   sudo apt update")
                    print("   sudo apt install -y docker.io")
                    print("   sudo systemctl start docker")
                    print("   sudo systemctl enable docker")
                    print("   sudo usermod -aG docker $USER")
                    print("   # Log out and back in\n")
                elif "fedora" in os_info or "rhel" in os_info:
                    print("Fedora/RHEL/CentOS:")
                    print("   sudo dnf install -y docker")
                    print("   sudo systemctl start docker")
                    print("   sudo systemctl enable docker")
                    print("   sudo usermod -aG docker $USER\n")
                else:
                    print("Visit: https://docs.docker.com/engine/install/\n")
            except:
                print("Visit: https://docs.docker.com/engine/install/\n")
        
        print("\n" + "="*80)
        print("‚ùå Please install Docker and re-run the script.")
        print("="*80)
        sys.exit(1)
    
    elif not docker_running:
        print("‚ö†Ô∏è  DOCKER IS NOT RUNNING")
        print("="*80)
        print("\nDocker is required for safe evaluation. Please start Docker:\n")
        if system == "Darwin":
            print("   ‚Üí Open Docker Desktop or OrbStack from Applications")
            print("   ‚Üí Wait for the status icon to show 'Docker is running'")
        elif system == "Linux":
            print("   ‚Üí sudo systemctl start docker")
        print("\nThen re-run this script.")
        print("="*80)
        
        response = input("\nContinue anyway? (Benchmark will run but without Docker isolation) [y/N]: ").strip().lower()
        if response != 'y':
            sys.exit(0)
        
        print("\n" + "="*80)
        print("‚ö†Ô∏è  Proceeding without Docker... (reduced safety)")
        print("="*80)
    
    return True

# Run prerequisites check immediately
check_prerequisites()

# -----------------------------------------------------------------------------
# Helper functions
# -----------------------------------------------------------------------------
def run(cmd, cwd=None, check=True, env=None, capture_output=False):
    """Execute command with optional output capture"""
    print(f"üîπ {' '.join(cmd)}")
    result = subprocess.run(cmd, cwd=cwd, check=check, env=env, 
                          capture_output=capture_output, text=True if capture_output else None)
    return result

def check_git():
    """Check if git is available"""
    try:
        subprocess.run(["git", "--version"], capture_output=True, check=True)
        return True
    except (FileNotFoundError, subprocess.CalledProcessError):
        print("‚ùå Git is not installed. Please install git first.")
        return False

def check_python():
    """Check Python version and return executable path"""
    major, minor = sys.version_info[:2]
    print(f"üêç Python version: {major}.{minor}")
    
    if major == 3 and minor >= 8:
        print("‚úÖ Python version is compatible")
        return sys.executable
    else:
        print(f"‚ùå Python {major}.{minor} is not supported. Please use Python 3.8+")
        sys.exit(1)

def log_step(step_name, status="START"):
    """Log step progress"""
    timestamp = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    print(f"\n{'='*60}")
    print(f"[{timestamp}] {status}: {step_name}")
    print(f"{'='*60}")

# -----------------------------------------------------------------------------
# Step 1: Environment Setup
# -----------------------------------------------------------------------------
log_step("Aider Polyglot Benchmark Setup")

print("üîç Checking system requirements...")
python_exe = check_python()
if not check_git():
    sys.exit(1)

# -----------------------------------------------------------------------------
# Step 2: Clone Aider Repository
# -----------------------------------------------------------------------------
log_step("Repository Setup")

aider_repo_url = "https://github.com/Aider-AI/aider.git"
aider_dir = Path("aider")

if not aider_dir.exists():
    print("üì• Cloning Aider repository...")
    run(["git", "clone", aider_repo_url])
else:
    print("‚úÖ Aider repository already exists")
    print("üîÑ Updating repository...")
    run(["git", "pull"], cwd=str(aider_dir))

# Create benchmarks directory
benchmarks_dir = aider_dir / "tmp.benchmarks"
benchmarks_dir.mkdir(exist_ok=True)
print(f"üìÅ Created benchmarks directory: {benchmarks_dir}")

# Clone polyglot benchmark repo
polyglot_repo_url = "https://github.com/Aider-AI/polyglot-benchmark"
polyglot_dir = benchmarks_dir / "polyglot-benchmark"

if not polyglot_dir.exists():
    print("üì• Cloning polyglot benchmark repository...")
    run(["git", "clone", polyglot_repo_url, str(polyglot_dir)])
else:
    print("‚úÖ Polyglot benchmark already exists")
    # Check if it's a valid git repository before trying to pull
    git_dir = polyglot_dir / ".git"
    if git_dir.exists():
        print("üîÑ Updating repository...")
        try:
            run(["git", "pull"], cwd=str(polyglot_dir))
        except subprocess.CalledProcessError:
            print("‚ö†Ô∏è  Git pull failed, repository may be corrupted")
            print("üîÑ Re-cloning repository...")
            import shutil
            shutil.rmtree(polyglot_dir)
            run(["git", "clone", polyglot_repo_url, str(polyglot_dir)])
    else:
        print("‚ö†Ô∏è  Directory exists but is not a git repository")
        print("üîÑ Re-cloning repository...")
        import shutil
        shutil.rmtree(polyglot_dir)
        run(["git", "clone", polyglot_repo_url, str(polyglot_dir)])

# -----------------------------------------------------------------------------
# Step 3: Docker Container Setup
# -----------------------------------------------------------------------------
log_step("Docker Container Setup")

# Check if Docker image already exists
print("üîç Checking for existing Docker image...")
try:
    result = run(["docker", "images", "-q", "aider-benchmark"], capture_output=True)
    if result.stdout.strip():
        print("‚úÖ Docker image 'aider-benchmark' already exists")
        print("   Skipping build step...")
    else:
        print("üî® Docker image not found, building now...")
        
        docker_build_script = aider_dir / "benchmark" / "docker_build.sh"
        if docker_build_script.exists():
            print("‚ö†Ô∏è  This may take several minutes on first run...")
            run(["chmod", "+x", str(docker_build_script)])
            
            # Try to build with the original script first
            try:
                run(["./benchmark/docker_build.sh"], cwd=str(aider_dir))
                print("‚úÖ Docker container built successfully")
            except subprocess.CalledProcessError as e:
                print("‚ö†Ô∏è  Standard Docker build failed, trying alternative approach...")
                print("   This is likely due to setuptools-scm version detection issues")
                
                # Alternative: Build Docker container with environment variable to bypass version detection
                print("üî® Building Docker container with version override...")
                env = os.environ.copy()
                env["SETUPTOOLS_SCM_PRETEND_VERSION"] = "0.86.2.dev"
                
                # Use docker build directly with environment variable
                docker_build_cmd = [
                    "docker", "build",
                    "--file", "benchmark/Dockerfile",
                    "--build-arg", "SETUPTOOLS_SCM_PRETEND_VERSION=0.86.2.dev",
                    "-t", "aider-benchmark",
                    "."
                ]
                
                try:
                    run(docker_build_cmd, cwd=str(aider_dir), env=env)
                    print("‚úÖ Docker container built successfully with version override")
                except subprocess.CalledProcessError:
                    print("‚ùå Docker build failed even with version override")
                    print("   Continuing without Docker container build...")
                    print("   The benchmark will attempt to run but may fail")
        else:
            print("‚ùå Docker build script not found at benchmark/docker_build.sh")
            print("   Please ensure you're in the correct directory")
            sys.exit(1)
except subprocess.CalledProcessError:
    print("‚ùå Failed to check for existing Docker images")
    print("   Proceeding with build attempt...")

# -----------------------------------------------------------------------------
# Step 4: Configuration Setup
# -----------------------------------------------------------------------------
log_step("Configuration Setup")

# Default configuration
config = {
    "edit_format": "whole",
    "threads": 10,
    "tries": 2,
    "exercises_dir": "polyglot-benchmark"
}

# Interactive configuration
print("‚öôÔ∏è  Benchmark Configuration")
print("=" * 40)

# Model selection - ask user for custom model
print("\nü§ñ Model Configuration:")
print("Enter the model name you want to benchmark.")
print("Examples: gpt-4, gpt-3.5-turbo, claude-3.5-sonnet, etc.")
config["model"] = input("Model name: ").strip()

if not config["model"]:
    print("‚ùå Model name is required!")
    sys.exit(1)

# API Configuration
print("\nüîë API Configuration:")

# Get API base URL (optional for standard OpenAI)
api_base = input("Enter your API base URL (leave empty for standard OpenAI, e.g., https://api.your-provider.com/v1): ").strip()

if api_base:
    # Custom API endpoint
    config["api_base"] = api_base
    
    # For custom endpoints, API key might be optional
    print("For custom API endpoints, authentication may be optional.")
    api_key = input("Enter your API key (leave empty if no authentication required): ").strip()
    
    if api_key:
        config["api_key"] = api_key
        print(f"‚úÖ Custom API configured with authentication:")
        print(f"   ‚Ä¢ Base URL: {api_base}")
        print(f"   ‚Ä¢ Model: {config['model']}")
        print(f"   ‚Ä¢ API Key: {'*' * (len(api_key) - 4) + api_key[-4:] if len(api_key) > 4 else '****'}")
    else:
        config["api_key"] = "dummy-key"  # LiteLLM requires some value
        print(f"‚úÖ Custom API configured without authentication:")
        print(f"   ‚Ä¢ Base URL: {api_base}")
        print(f"   ‚Ä¢ Model: {config['model']}")
        print(f"   ‚Ä¢ Authentication: None")
else:
    # Standard providers - API key is required
    print("For standard providers, you need to provide API credentials.")
    api_key = input("Enter your API key: ").strip()
    if not api_key:
        print("‚ùå API key is required for standard providers!")
        sys.exit(1)
    
    # Detect provider based on model name
    if "gpt" in config["model"].lower() or "openai" in config["model"].lower():
        provider = "OpenAI"
        env_var = "OPENAI_API_KEY"
    elif "claude" in config["model"].lower() or "anthropic" in config["model"].lower():
        provider = "Anthropic"
        env_var = "ANTHROPIC_API_KEY"
    elif "gemini" in config["model"].lower() or "google" in config["model"].lower():
        provider = "Google"
        env_var = "GOOGLE_API_KEY"
    else:
        provider = "OpenAI (default)"
        env_var = "OPENAI_API_KEY"
    
    config["api_key"] = api_key
    config["env_var"] = env_var
    
    print(f"‚úÖ API key configured for {provider}")
    print(f"   ‚Ä¢ Key: {'*' * (len(api_key) - 4) + api_key[-4:] if len(api_key) > 4 else '****'}")

# Edit format selection
print("\n‚úèÔ∏è  Edit Format:")
print("1. whole (default - replace entire file)")
print("2. diff (generate diffs)")
print("3. udiff (unified diffs)")

edit_choice = input("Choose edit format (1-3, default: 1): ").strip() or "1"
if edit_choice == "2":
    config["edit_format"] = "diff"
elif edit_choice == "3":
    config["edit_format"] = "udiff"

# Thread configuration
threads_input = input(f"Number of parallel threads (default: {config['threads']}): ").strip()
if threads_input:
    config["threads"] = int(threads_input)

# Number of tests
num_tests = input("Number of tests to run (default: all): ").strip()
if num_tests:
    config["num_tests"] = int(num_tests)

# Language selection
print("\nüåê Language Selection:")
print("Choose which programming language to test:")
print("1. All languages (default)")
print("2. C++")
print("3. Go")
print("4. Java")
print("5. JavaScript")
print("6. Python")
print("7. Rust")

language_choice = input("Choose option (1-7, default: 1): ").strip() or "1"

# Map language choices to directory names
language_map = {
    "2": "cpp",
    "3": "go", 
    "4": "java",
    "5": "javascript",
    "6": "python",
    "7": "rust",
}

if language_choice in language_map:
    selected_lang = language_map[language_choice]
    config["keywords"] = selected_lang
    print(f"‚úÖ Selected language: {selected_lang}")
else:
    print("‚úÖ Using all languages")

# Additional keywords filter
additional_keywords = input("Additional keyword filter (optional): ").strip()
if additional_keywords:
    if "keywords" in config:
        config["keywords"] = f"{config['keywords']}.*{additional_keywords}"
    else:
        config["keywords"] = additional_keywords

# Model settings file - Auto-apply context fix for GLM models
model_settings = input("Path to model settings YAML file (optional, press Enter for auto GLM context fix): ").strip()

if model_settings and Path(model_settings).exists():
    config["model_settings"] = model_settings
    print(f"‚úÖ Using custom model settings: {model_settings}")
elif "glm" in config["model"].lower():
    # Auto-apply GLM context fix settings
    glm_fix_file = Path("glm-context-fix.yaml")
    if glm_fix_file.exists():
        config["model_settings"] = str(glm_fix_file)
        print("üõ°Ô∏è  Auto-applying GLM context exhaustion fix")
        print(f"   Settings file: {glm_fix_file}")
        print("   This prevents context window crashes by limiting output tokens")
    else:
        print("‚ö†Ô∏è  GLM model detected but context fix file not found")
        print("   Consider creating glm-context-fix.yaml for optimal performance")
else:
    # For other models, check if user wants to use the GLM fix anyway
    if input("Apply context exhaustion protection? (recommended) [Y/n]: ").strip().lower() not in ['n', 'no']:
        glm_fix_file = Path("glm-context-fix.yaml")
        if glm_fix_file.exists():
            config["model_settings"] = str(glm_fix_file)
            print("üõ°Ô∏è  Applying context exhaustion protection")
            print(f"   Settings file: {glm_fix_file}")
        else:
            print("‚ö†Ô∏è  Context fix file not found, proceeding without protection")

print(f"\n‚úÖ Configuration:")
for key, value in config.items():
    print(f"   {key}: {value}")

# -----------------------------------------------------------------------------
# Step 5: Run Benchmark
# -----------------------------------------------------------------------------
log_step("Running Benchmark")

# Generate run name
timestamp = datetime.datetime.now().strftime('%Y-%m-%d-%H-%M-%S')
run_name = f"{timestamp}--{config['model'].replace('/', '-')}-{config['edit_format']}"

print(f"üöÄ Starting benchmark: {run_name}")
print(f"ü§ñ Model: {config['model']}")
print(f"‚úèÔ∏è  Edit format: {config['edit_format']}")
print(f"üßµ Threads: {config['threads']}")

# Prepare benchmark command to run inside Docker
benchmark_cmd_parts = [
    "pip install -e .[dev] &&",
    "./benchmark/benchmark.py",
    run_name,
    "--model", config["model"],
    "--edit-format", config["edit_format"],
    "--threads", str(config["threads"]),
    "--tries", str(config["tries"]),
    "--exercises-dir", config["exercises_dir"]
]

# Add optional parameters
if "num_tests" in config:
    benchmark_cmd_parts.extend(["--num-tests", str(config["num_tests"])])

if "keywords" in config:
    benchmark_cmd_parts.extend(["--keywords", config["keywords"]])

if "model_settings" in config:
    benchmark_cmd_parts.extend(["--read-model-settings", config["model_settings"]])

# Join command parts
docker_benchmark_cmd = " ".join(benchmark_cmd_parts)

# For custom APIs, use the model name directly without any prefix
if "api_base" in config:
    model_name = config['model']
    print(f"üîß Custom API endpoint detected, using model name directly: {model_name}")
    # No prefix needed for custom models - use the exact model name as provided

print(f"üìä Docker command: {docker_benchmark_cmd}")

# Check if Docker script exists
docker_script = aider_dir / "benchmark" / "docker.sh"
if not docker_script.exists():
    print("‚ùå Docker script not found at benchmark/docker.sh")
    sys.exit(1)

print("üê≥ Launching Docker container and running benchmark...")
print("‚è≥ This may take a while depending on the number of tests...")
run(["chmod", "+x", str(docker_script)])

# Instead of using docker.sh directly, we'll run docker with the same parameters
# but execute our command directly instead of launching interactive bash
print(f"üîπ Running Docker container with command: {docker_benchmark_cmd}")
docker_run_cmd = [
    "docker", "run",
    "--rm",
    "--memory=12g",
    "--memory-swap=12g",
    "--add-host=host.docker.internal:host-gateway",
    "-v", f"{aider_dir.absolute()}:/aider",
    "-v", f"{aider_dir.absolute()}/tmp.benchmarks/.:/benchmarks",
    "-e", "AIDER_DOCKER=1",
    "-e", "AIDER_BENCHMARK_DIR=/benchmarks",
    "-w", "/aider",
]

# Add API credentials to Docker environment
if "api_base" in config:
    # Custom API endpoint - use openai/ prefix to force LiteLLM to use OpenAI client
    model_name = config['model']
    
    # Set OpenAI client environment variables
    docker_run_cmd.extend(["-e", f"OPENAI_API_KEY={config['api_key']}"])
    docker_run_cmd.extend(["-e", f"OPENAI_API_BASE={config['api_base']}"])
    
    # Force LiteLLM to recognize this as an OpenAI model by prefixing with 'openai/'
    # This tells LiteLLM to use the OpenAI client with your custom endpoint
    # For model names starting with '/', create a simple alias to avoid LiteLLM processing issues
    if model_name.startswith('/'):
        # Create a simple alias without special characters
        model_alias = model_name.replace('/', '_').replace('-', '_')
        prefixed_model = f"openai/{model_alias}"
        # Configure LiteLLM to map the alias to the actual model name
        litellm_config = {
            "model_list": [
                {
                    "model_name": model_alias,
                    "litellm_params": {
                        "model": model_name,
                        "api_base": config['api_base'],
                        "api_key": config['api_key']
                    }
                }
            ]
        }
        # Write config to a temporary file and mount it
        import json
        config_content = json.dumps(litellm_config, indent=2)
        docker_run_cmd.extend(["-e", f"LITELLM_CONFIG_JSON={config_content}"])
    else:
        prefixed_model = f"openai/{model_name}"
    
    # Update the benchmark command to use the prefixed model name
    # Need to handle potential shell escaping issues with model names containing special characters
    old_model_arg = f"--model {model_name}"
    new_model_arg = f"--model {prefixed_model}"
    docker_benchmark_cmd = docker_benchmark_cmd.replace(old_model_arg, new_model_arg)
    
    # Debug: Print the replacement to verify it worked
    print(f"üîÑ Model name replacement:")
    print(f"   Original: {old_model_arg}")
    print(f"   New: {new_model_arg}")
    print(f"   Command updated: {'‚úÖ' if new_model_arg in docker_benchmark_cmd else '‚ùå'}")
    
    # Disable model metadata lookup to prevent LiteLLM from trying to validate the model
    docker_run_cmd.extend(["-e", "AIDER_MODEL_METADATA_FILE=/dev/null"])
    docker_run_cmd.extend(["-e", "LITELLM_LOG=DEBUG"])
    
    print(f"üîë Using custom API: {config['api_base']}")
    print(f"üîß Model prefixed for LiteLLM: {prefixed_model}")
    print(f"üîÑ LiteLLM will use OpenAI client with your custom endpoint")
    print(f"üö´ Model metadata lookup disabled")
else:
    # Standard provider
    if "env_var" in config:
        docker_run_cmd.extend(["-e", f"{config['env_var']}={config['api_key']}"])
        print(f"üîë Using {config.get('env_var', 'API_KEY')}")
    else:
        # Fallback to OPENAI_API_KEY
        docker_run_cmd.extend(["-e", f"OPENAI_API_KEY={config['api_key']}"])

# Add the container image and command
docker_run_cmd.extend([
    "aider-benchmark",
    "bash", "-c", docker_benchmark_cmd
])

# Add any additional environment variables that might be needed from system
for env_var in ["ANTHROPIC_API_KEY", "GOOGLE_API_KEY", "AZURE_OPENAI_API_KEY", "COHERE_API_KEY"]:
    if env_var in os.environ and env_var != config.get('env_var'):
        docker_run_cmd.extend(["-e", f"{env_var}={os.environ[env_var]}"])

run(docker_run_cmd, cwd=str(aider_dir))

# -----------------------------------------------------------------------------
# Step 6: Generate Report
# -----------------------------------------------------------------------------
log_step("Generating Report")

# Find the results directory
results_pattern = f"tmp.benchmarks/*{run_name}*"
results_dirs = list(aider_dir.glob(results_pattern))

if results_dirs:
    results_dir = results_dirs[0]
    print(f"üìä Found results in: {results_dir}")
    
    # The benchmark already generates stats during execution, so we just need to read them
    # Try to read and display the stats
    try:
        # Look for YAML stats file
        stats_files = list(results_dir.glob("*.yaml"))
        if not stats_files:
            stats_files = list(results_dir.glob("*.yml"))
        
        if stats_files:
            stats_file = stats_files[0]
            with open(stats_file, 'r') as f:
                stats_content = f.read()
                print(f"\nüìà Benchmark Results Summary:")
                print("=" * 50)
                print(stats_content)
                print("=" * 50)
        else:
            print("üìä Stats file not found, checking results directory...")
            # List files in results directory
            result_files = list(results_dir.glob("*"))
            print(f"   Files found: {[f.name for f in result_files]}")
            
            # Check if benchmark completed successfully
            json_files = list(results_dir.glob("*/.aider.results.json"))
            if json_files:
                print(f"   Found {len(json_files)} test result files")
                # Read a sample result to show status
                try:
                    with open(json_files[0], 'r') as f:
                        sample_result = json.load(f)
                        print(f"   Sample result: {sample_result.get('testcase', 'unknown')} - "
                              f"Pass rate: {sum(sample_result.get('tests_outcomes', [])) / len(sample_result.get('tests_outcomes', [1])) * 100:.1f}%")
                except Exception as e:
                    print(f"   Could not read sample result: {e}")
    except Exception as e:
        print(f"‚ö†Ô∏è  Could not read stats: {e}")
else:
    print("‚ö†Ô∏è  No results directory found")

# -----------------------------------------------------------------------------
# Step 7: Summary
# -----------------------------------------------------------------------------
log_step("Benchmark Complete", "FINISH")

print("üéâ Aider Polyglot Benchmark completed successfully!")
print("\nüìä Summary:")
print(f"   ‚Ä¢ Run name: {run_name}")
print(f"   ‚Ä¢ Model: {config['model']}")
print(f"   ‚Ä¢ Edit format: {config['edit_format']}")
print(f"   ‚Ä¢ Threads: {config['threads']}")

if results_dirs:
    results_dir = results_dirs[0]
    print(f"   ‚Ä¢ Results: {results_dir}")

print(f"\nüîç View detailed results:")
print(f"   ‚Ä¢ Results directory: {results_dirs[0] if results_dirs else 'Not found'}")
print(f"   ‚Ä¢ Run stats command: ./benchmark/benchmark.py --stats {results_dirs[0] if results_dirs else 'RESULTS_DIR'}")

print(f"\nüí° Next steps:")
print(f"   ‚Ä¢ Review results in the results directory")
print(f"   ‚Ä¢ Compare with other model benchmarks")
print(f"   ‚Ä¢ Run again with different models or settings")
print(f"   ‚Ä¢ Submit results to Aider leaderboard if desired")

print(f"\n‚úÖ Benchmark run '{run_name}' completed!")
