
================================================================================
ğŸ“Š EVALUATION SUMMARY
================================================================================

âœ… Total Items Evaluated: 263
ğŸ“ Evaluation Mode: Triple-Output Evaluation (3 outputs per datapoint)

================================================================================
ğŸ”„ TRIPLE-OUTPUT EVALUATION RESULTS
================================================================================

ğŸ“‹ Breakdown by Task Type:

  ğŸ“‚ code_debugging (65 items)
     â€¢ Average Score:      0.720
     â€¢ Std Deviation:      0.225
     â€¢ Min Score:          0.203
     â€¢ Max Score:          1.000

  ğŸ“‚ code_generation (102 items)
     â€¢ Average Score:      0.643
     â€¢ Std Deviation:      0.197
     â€¢ Min Score:          0.240
     â€¢ Max Score:          0.995

  ğŸ“‚ code_understanding (96 items)
     â€¢ Average Score:      0.718
     â€¢ Std Deviation:      0.142
     â€¢ Min Score:          0.237
     â€¢ Max Score:          0.920

================================================================================
ğŸ¯ OVERALL TRIPLE-OUTPUT RESULTS
================================================================================
Average Score Across All Items:  0.689
Standard Deviation:              0.190
Minimum Average Score:           0.203
Maximum Average Score:           1.000

ğŸ“ˆ Performance Distribution:
   High Performance (â‰¥0.8):     97/263 (36.9%)
   Good Performance (0.6-0.8):  83/263 (31.6%)
   Fair Performance (0.4-0.6):  64/263 (24.3%)
   Poor Performance (<0.4):     19/263 (7.2%)

ğŸ“Š Model shows moderate consistency across outputs.
================================================================================