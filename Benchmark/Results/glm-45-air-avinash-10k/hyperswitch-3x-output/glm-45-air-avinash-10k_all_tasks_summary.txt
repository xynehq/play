
================================================================================
ğŸ“Š EVALUATION SUMMARY
================================================================================

âœ… Total Items Evaluated: 263
ğŸ“ Evaluation Mode: Triple-Output Evaluation (3 outputs per datapoint)

================================================================================
ğŸ”„ TRIPLE-OUTPUT EVALUATION RESULTS
================================================================================

ğŸ“‹ Breakdown by Task Type:

  ğŸ“‚ code_debugging (65 items)
     â€¢ Average Score:      0.765
     â€¢ Std Deviation:      0.205
     â€¢ Min Score:          0.096
     â€¢ Max Score:          0.992

  ğŸ“‚ code_generation (102 items)
     â€¢ Average Score:      0.673
     â€¢ Std Deviation:      0.214
     â€¢ Min Score:          0.233
     â€¢ Max Score:          1.000

  ğŸ“‚ code_understanding (96 items)
     â€¢ Average Score:      0.699
     â€¢ Std Deviation:      0.139
     â€¢ Min Score:          0.250
     â€¢ Max Score:          0.894

================================================================================
ğŸ¯ OVERALL TRIPLE-OUTPUT RESULTS
================================================================================
Average Score Across All Items:  0.705
Standard Deviation:              0.191
Minimum Average Score:           0.096
Maximum Average Score:           1.000

ğŸ“ˆ Performance Distribution:
   High Performance (â‰¥0.8):     98/263 (37.3%)
   Good Performance (0.6-0.8):  89/263 (33.8%)
   Fair Performance (0.4-0.6):  57/263 (21.7%)
   Poor Performance (<0.4):     19/263 (7.2%)

âœ¨ Model shows consistent good performance across multiple outputs!
================================================================================