
================================================================================
ğŸ“Š EVALUATION SUMMARY
================================================================================

âœ… Total Items Evaluated: 263
ğŸ“ Evaluation Mode: Triple-Output Evaluation (3 outputs per datapoint)

================================================================================
ğŸ”„ TRIPLE-OUTPUT EVALUATION RESULTS
================================================================================

ğŸ“‹ Breakdown by Task Type:

  ğŸ“‚ code_debugging (65 items)
     â€¢ Average Score:      0.762
     â€¢ Std Deviation:      0.203
     â€¢ Min Score:          0.200
     â€¢ Max Score:          0.996

  ğŸ“‚ code_generation (102 items)
     â€¢ Average Score:      0.664
     â€¢ Std Deviation:      0.212
     â€¢ Min Score:          0.245
     â€¢ Max Score:          1.000

  ğŸ“‚ code_understanding (96 items)
     â€¢ Average Score:      0.678
     â€¢ Std Deviation:      0.138
     â€¢ Min Score:          0.250
     â€¢ Max Score:          0.904

================================================================================
ğŸ¯ OVERALL TRIPLE-OUTPUT RESULTS
================================================================================
Average Score Across All Items:  0.693
Standard Deviation:              0.190
Minimum Average Score:           0.200
Maximum Average Score:           1.000

ğŸ“ˆ Performance Distribution:
   High Performance (â‰¥0.8):     91/263 (34.6%)
   Good Performance (0.6-0.8):  90/263 (34.2%)
   Fair Performance (0.4-0.6):  65/263 (24.7%)
   Poor Performance (<0.4):     17/263 (6.5%)

ğŸ“Š Model shows moderate consistency across outputs.
================================================================================