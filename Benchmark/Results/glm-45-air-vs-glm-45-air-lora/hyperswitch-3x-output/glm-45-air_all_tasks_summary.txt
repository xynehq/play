
================================================================================
ğŸ“Š EVALUATION SUMMARY
================================================================================

âœ… Total Items Evaluated: 263
ğŸ“ Evaluation Mode: Triple-Output Evaluation (3 outputs per datapoint)

================================================================================
ğŸ”„ TRIPLE-OUTPUT EVALUATION RESULTS
================================================================================

ğŸ“‹ Breakdown by Task Type:

  ğŸ“‚ code_debugging (65 items)
     â€¢ Average Score:      0.744
     â€¢ Std Deviation:      0.205
     â€¢ Min Score:          0.138
     â€¢ Max Score:          0.996

  ğŸ“‚ code_generation (102 items)
     â€¢ Average Score:      0.650
     â€¢ Std Deviation:      0.215
     â€¢ Min Score:          0.200
     â€¢ Max Score:          1.000

  ğŸ“‚ code_understanding (96 items)
     â€¢ Average Score:      0.676
     â€¢ Std Deviation:      0.137
     â€¢ Min Score:          0.299
     â€¢ Max Score:          0.900

================================================================================
ğŸ¯ OVERALL TRIPLE-OUTPUT RESULTS
================================================================================
Average Score Across All Items:  0.682
Standard Deviation:              0.191
Minimum Average Score:           0.138
Maximum Average Score:           1.000

ğŸ“ˆ Performance Distribution:
   High Performance (â‰¥0.8):     81/263 (30.8%)
   Good Performance (0.6-0.8):  100/263 (38.0%)
   Fair Performance (0.4-0.6):  61/263 (23.2%)
   Poor Performance (<0.4):     21/263 (8.0%)

ğŸ“Š Model shows moderate consistency across outputs.
================================================================================