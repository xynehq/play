
================================================================================
ğŸ“Š EVALUATION SUMMARY
================================================================================

âœ… Total Items Evaluated: 235
ğŸ“ Evaluation Mode: Dual-Model Comparison

================================================================================
ğŸ¤– DUAL-OUTPUT COMPARISON RESULTS
================================================================================

ğŸ“‹ Breakdown by Task Type:

  ğŸ“‚ code_debugging (65 items)
     â€¢ Output 1 Avg Score: 0.731
     â€¢ Output 2 Avg Score: 0.687
     â€¢ Difference:         -0.044
     â€¢ Wins: O1=32, O2=30, Ties=3

  ğŸ“‚ code_generation (74 items)
     â€¢ Output 1 Avg Score: 0.625
     â€¢ Output 2 Avg Score: 0.703
     â€¢ Difference:         +0.078
     â€¢ Wins: O1=20, O2=43, Ties=11

  ğŸ“‚ code_understanding (96 items)
     â€¢ Output 1 Avg Score: 0.674
     â€¢ Output 2 Avg Score: 0.712
     â€¢ Difference:         +0.038
     â€¢ Wins: O1=22, O2=56, Ties=18

================================================================================
ğŸ¯ OVERALL COMPARISON
================================================================================
Output 1 Average Score:  0.674
Output 2 Average Score:  0.702
Average Difference:      +0.028

ğŸ† Win Record:
   Output 1 Wins:    74/235 (31.5%)
   Output 2 Wins:    129/235 (54.9%)
   Ties:             32/235 (13.6%)

âœ¨ Output 2 performs better overall!

================================================================================
ğŸ“ BREAKDOWN BY CATEGORY
================================================================================

  repo_specific (168 items):
     Difference: +0.033
     Wins: O1=47, O2=89

  generic (62 items):
     Difference: +0.013
     Wins: O1=24, O2=29

  complex_logic (5 items):
     Difference: +0.050
     Wins: O1=1, O2=4
================================================================================