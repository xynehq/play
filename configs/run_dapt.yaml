include: configs/config_base.yaml

# DAPT-specific overrides
task_mode: cpt_mixed             # Enable mixed CPT + instruction training

model:
  name: Qwen/Qwen2.5-1.5B-Instruct  # Use smaller model for testing
  max_seq_len: 1024              # Moderate sequence length for testing

tuning:
  backend: bnb
  lora:
    r: 16                        # Smaller rank for DAPT
    alpha: 32
    target_modules: [q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj]

# Mix CPT + anchor at train-time
datasets:
  - name: cpt
    path: data/processed/cpt.jsonl
    type: cpt
    weight: 0.9
  - name: anchor_instr
    path: data/processed/anchor_instr.jsonl
    type: chat
    weight: 0.1

train:
  epochs: 1                      # Single epoch for DAPT
  learning_rate: 1.0e-4          # Lower learning rate
  warmup_ratio: 0.03
  evaluation_strategy: "no"      # No evaluation for DAPT
  eval_strategy: "no"            # Ensure both variants are set
  save_strategy: "steps"         # Keep save strategy as steps
  load_best_model_at_end: false
  logging_steps: 25
  save_steps: 500
  output_dir: outputs/run-dapt