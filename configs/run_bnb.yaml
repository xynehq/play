include: configs/config_base.yaml

# Model configuration for Gemma 3 27B internal docs
model:
  name: google/gemma-3-27b-it
  max_seq_len: 8192    # Extended for full docs + context + query
  type: causal

# Tuning for generalization (not overfitting)
tuning:
  mode: qlora
  backend: bnb         # BitsAndBytes backend
  lora:
    r: 32              # Moderate rank for generalization
    alpha: 64          # 2x rank
    dropout: 0.1       # Higher dropout to prevent overfitting
    target_modules: ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]

# Training optimized for production RAG - SPEED OPTIMIZED
train:
  epochs: 2            # Reduced to 2 epochs for faster training
  learning_rate: 1e-4  # Higher LR for faster convergence
  weight_decay: 0.01
  warmup_ratio: 0.05   # Shorter warmup for speed
  lr_scheduler_type: linear  # Simpler scheduler for speed
  
  # H200 speed optimized settings
  bf16: true           # H200 supports bf16 natively
  fp16: false
  gradient_checkpointing: false  # Disabled for speed (H200 has enough VRAM)
  batch_size: 4        # Increased batch size for H200
  grad_accum: 2        # Effective batch = 8
  
  # Checkpoint after every epoch
  evaluation_strategy: epoch
  save_strategy: epoch
  save_total_limit: 4  # Keep all 4 epoch checkpoints
  load_best_model_at_end: true
  metric_for_best_model: eval_loss
  greater_is_better: false
  
  output_dir: outputs/run-bnb-gemma27b

# Data configuration for Gemma template
data:
  template_path: chat_templates/gemma.jinja
  split:
    train_ratio: 0.8   # More training data
    val_ratio: 0.15    # Validation for early stopping
    test_ratio: 0.05   # Small test set

# Production RAG configuration
production_rag:
  system_prompt_path: prompts/juspay_system_prompt.txt
  context_docs_path: data/raw/juspay_docs_context.txt
  enable_context_injection: true
  context_sample_ratio: 0.2  # 20% of samples get context injection
