include: configs/config_base.yaml

# Model configuration for Gemma 3 27B internal docs
model:
  name: google/gemma-3-27b-it
  max_seq_len: 8192    # Extended for full docs + context + query
  type: causal

# Tuning for generalization (not overfitting)
tuning:
  mode: qlora
  backend: bnb         # BitsAndBytes backend
  lora:
    r: 32              # Moderate rank for generalization
    alpha: 64          # 2x rank
    dropout: 0.1       # Higher dropout to prevent overfitting
    target_modules: ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]

# Training optimized for production RAG
train:
  epochs: 4            # 4 epochs as requested
  learning_rate: 5e-5  # Lower LR for stable, generalizable learning
  weight_decay: 0.01
  warmup_ratio: 0.1    # Longer warmup for stability
  lr_scheduler_type: cosine_with_restarts  # Better for generalization
  
  # H200 optimized settings
  bf16: true           # H200 supports bf16 natively
  fp16: false
  gradient_checkpointing: true
  batch_size: 2        # Conservative for 4K context
  grad_accum: 4        # Effective batch = 8
  
  # Checkpoint after every epoch
  evaluation_strategy: epoch
  save_strategy: epoch
  save_total_limit: 4  # Keep all 4 epoch checkpoints
  load_best_model_at_end: true
  metric_for_best_model: eval_loss
  greater_is_better: false
  
  output_dir: outputs/run-bnb-gemma27b

# Data configuration for Gemma template
data:
  template_path: chat_templates/gemma.jinja
  split:
    train_ratio: 0.8   # More training data
    val_ratio: 0.15    # Validation for early stopping
    test_ratio: 0.05   # Small test set

# Production RAG configuration
production_rag:
  system_prompt_path: prompts/juspay_system_prompt.txt
  context_docs_path: data/raw/juspay_docs_context.txt
  enable_context_injection: true
