# ğŸš€ SFT-Play v1.0 â€” Plug-and-Play LoRA/QLoRA Fine-Tuning in 8GB VRAM

**SFT-Play** is a reusable, beginner-friendly, 8GB-VRAMâ€“friendly environment for supervised fine-tuning (SFT) of LLMs with **LoRA**/**QLoRA**. It's built for stability, speed, and minimal setup â€” so you can focus on experimentation instead of debugging.

---

## âœ¨ **What's New in v1.0**

### ğŸ”¹ Stable Backends with Auto-Fallback

* **BitsAndBytes** as the default stable backend
* **Unsloth** available for experimentation
* Automatic fallback to BitsAndBytes when Unsloth + XFormers incompatibilities are detected
* Backend-specific configs (`run_bnb.yaml`, `run_unsloth.yaml`) to avoid cross-contamination

### ğŸ”¹ Training Safety Features

* Backend stamping to prevent mismatched resume runs
* Config validation for precision, LoRA params, and paths
* Resume protection to avoid corrupted checkpoints
* Config defaults tuned for 8GB VRAM QLoRA

### ğŸ”¹ Logging & Monitoring

* **TensorBoard** integration out of the box
* Logging of `train/loss`, `eval/loss`, `train/lr`, and metrics
* Preconfigured Makefile commands:

  ```bash
  make train-bnb-tb     # Stable + TensorBoard
  make train-unsloth-tb # Experimentation + Fallback
  ```

### ğŸ”¹ Data & Templates

* Supports `system/user/assistant` structured chat data
* Built-in Jinja templating for instruction formatting
* CLI tools for:
  * Processing raw â†’ structured data
  * Adding style prompts
  * Rendering templates

### ğŸ”¹ **High-Quality Inference Engine** â­ NEW

* **Proper Stopping Conditions**: Forces stop at EOS tokens and chat template boundaries
* **Clean Output Extraction**: Strips everything after first `<|assistant|>` â†’ `<|user|>` boundary
* **Template Consistency**: Matches training template format exactly during inference
* **Single-Turn Generation**: Prevents multi-turn conversation bleed-through

**Example of clean inference:**
```python
# Raw generation might include:
# "<|system|>You are helpful</|system|><|user|>Hello</|user|><|assistant|>Hi there!<|user|>..."

# Cleaned output extracts only:
# "Hi there!"
```

### ğŸ”¹ Inference & Evaluation

* Clean single-turn inference with:
  ```bash
  make infer
  ```
* Evaluation script with ROUGE-L, SARI, EM, schema compliance
* LoRA merge script to export full FP16 model

### ğŸ”¹ **Complete Automation System**

* **20+ Makefile commands** for every aspect of the pipeline
* **Interactive setup** with `./workflows/quick_start.sh`
* **Batch processing** automation
* **Comprehensive validation** with `make check`

**Key automation commands:**
```bash
make install                 # Install dependencies (auto-detects uv/pip)
make setup-dirs             # Create all necessary directories
make full-pipeline          # Complete data processing pipeline
make train-with-tb          # Train with TensorBoard monitoring
make eval                   # Evaluate on validation set
make infer                  # Interactive inference
make merge                  # Merge LoRA adapters to FP16 model
```

### ğŸ”¹ **Model Management**

* **Automatic model downloading** from Hugging Face Hub
* **Local model caching** to avoid re-downloads
* **Hugging Face token integration** with multiple auth methods
* **Offline mode support** after initial download

---

## ğŸ“‚ **Folder Structure**

```
sft-play/
â”œâ”€ configs/              # Base + run configs
â”‚  â”œâ”€ config_base.yaml   # Reusable defaults
â”‚  â”œâ”€ config_run.yaml    # Per-run overrides
â”‚  â”œâ”€ run_bnb.yaml      # BitsAndBytes backend config
â”‚  â””â”€ run_unsloth.yaml  # Unsloth backend config
â”œâ”€ data/                 # Raw + processed datasets
â”‚  â”œâ”€ raw/              # Input sources (json/csv/jsonl)
â”‚  â”œâ”€ processed/        # Structured chat data
â”‚  â””â”€ rendered/         # Materialized templates
â”œâ”€ chat_templates/       # Jinja templates
â”‚  â””â”€ default.jinja     # Default chat template
â”œâ”€ scripts/              # Train, eval, infer, merge
â”‚  â”œâ”€ train.py          # QLoRA/LoRA/Full training
â”‚  â”œâ”€ eval.py           # Comprehensive evaluation
â”‚  â”œâ”€ infer.py          # High-quality inference
â”‚  â”œâ”€ process_data.py   # Data processing pipeline
â”‚  â””â”€ merge_lora.py     # Adapter merging
â”œâ”€ outputs/              # Checkpoints & logs
â”œâ”€ adapters/             # LoRA adapters (~50-200MB)
â”œâ”€ workflows/            # Automation scripts
â”œâ”€ Makefile             # Complete automation
â””â”€ requirements.txt
```

---

## âœ… **Why v1.0 is Ready**

* âœ… Runs QLoRA on RTX 4060 (8GB VRAM) without OOM
* âœ… No manual debugging for XFormers/Unsloth issues
* âœ… Fully documented setup and commands
* âœ… High-quality inference with proper stopping
* âœ… Complete automation system
* âœ… Ready for public use as a **starter kit** for SFT

---

## ğŸš€ **Quick Start**

### Option 1: One-Command Setup (Recommended)

```bash
git clone https://github.com/your-username/sft-play
cd sft-play
./workflows/quick_start.sh
```

This interactive script will:
- Install dependencies (auto-detects uv or pip)
- Create all necessary directories
- Generate sample data if none exists
- Process data through the complete pipeline
- Guide you to training

### Option 2: Manual Setup

```bash
git clone https://github.com/your-username/sft-play
cd sft-play
pip install -r requirements.txt

make setup-dirs         # Create directories
make process           # Prepare dataset
make check             # Validate setup
make train-bnb-tb      # Start stable QLoRA training with TensorBoard
```

### Option 3: Advanced Workflow

```bash
make install && make setup-dirs
make process
make style STYLE="Be concise and professional"
make check
make train CONFIG=configs/run_bnb.yaml
make eval-test
make merge && make merge-test
```

---

## ğŸ¯ **Supported Models & Hardware**

### **Tested Models:**
- Qwen/Qwen2.5-3B-Instruct
- Meta-Llama models (with proper tokens)
- Mistral models
- Any HuggingFace causal LM

### **Hardware Requirements:**
- **Minimum**: 8GB VRAM (RTX 4060, RTX 3070, etc.)
- **Recommended**: 12GB+ VRAM for larger models
- **CPU**: Any modern CPU with 16GB+ RAM

### **VRAM Usage Examples:**
- Qwen2.5-3B + QLoRA + BitsAndBytes: ~6.5GB VRAM
- Llama-7B + QLoRA + BitsAndBytes: ~7.8GB VRAM

---

## ğŸ› ï¸ **Configuration Examples**

### Stable Training (Recommended)
```yaml
# configs/run_bnb.yaml
include: configs/config_base.yaml

tuning:
  backend: bnb             # BitsAndBytes backend
  mode: qlora

train:
  bf16: false              # BitsAndBytes works best with fp16
  fp16: true
  output_dir: outputs/run-bnb
```

### Experimental Training
```yaml
# configs/run_unsloth.yaml
include: configs/config_base.yaml

tuning:
  backend: unsloth         # Unsloth backend (auto-fallback to bnb)
  mode: qlora

train:
  bf16: true               # Unsloth works better with bfloat16
  fp16: false
  output_dir: outputs/run-unsloth
```

---

## ğŸ“Š **Performance & Quality**

### **Training Performance:**
- **Memory Efficient**: Only saves LoRA adapters (~50-200MB vs full model GB)
- **VRAM Optimized**: Auto-tunes batch size and gradient accumulation
- **Fast Resume**: Lightweight checkpoints enable quick restarts

### **Inference Quality:**
- **Clean Output**: Proper template boundary detection
- **Consistent Format**: Training-inference template matching
- **Single-Turn Focus**: Prevents multi-turn conversation bleed
- **Production Ready**: High-quality responses suitable for deployment

### **Automation Benefits:**
- **Zero Config**: Works out of the box with sensible defaults
- **Error Prevention**: Comprehensive validation and safety checks
- **Developer Friendly**: 20+ automation commands for every workflow

---

## ğŸ’¡ **Pro Tips**

* **For Maximum Stability**: Use `make train-bnb-tb`
* **For Experimentation**: Try `make train-unsloth-tb` (auto-fallback included)
* **For Quick Validation**: Use `make check` before training
* **For Clean Inference**: The new inference engine handles all edge cases
* **For Automation**: Explore `make help` for all available commands

---

## ğŸ”§ **Troubleshooting**

### Common Issues Resolved in v1.0:

* âœ… **XFormers Compatibility**: Auto-fallback prevents crashes
* âœ… **Backend Confusion**: Separate configs prevent cross-contamination
* âœ… **Memory Issues**: Auto-tuning handles VRAM constraints
* âœ… **Template Mismatches**: Inference engine ensures consistency
* âœ… **Setup Complexity**: One-command setup and comprehensive automation

### Quick Fixes:
```bash
make check                    # Validate everything
make clean && make setup-dirs # Reset if needed
./workflows/quick_start.sh    # Guided setup
```

---

## ğŸ“š **Documentation**

- **README.md** - Complete usage guide with examples
- **AUTOMATION_GUIDE.md** - Detailed automation system documentation
- **SETUP_DOCUMENTATION.md** - Complete project setup guide
- **This Release Note** - What's new in v1.0

---

## ğŸ‰ **Ready to Fine-Tune?**

SFT-Play v1.0 is production-ready for:
- **AI Hobbyists** â€” Fine-tune on your own hardware
- **Researchers** â€” Quick experimentation before scaling
- **Educators** â€” Teaching LLM fine-tuning
- **Developers** â€” Prototyping AI features
- **Open Source Contributors** â€” Building and sharing models

**Get started now:**
```bash
git clone https://github.com/your-username/sft-play
cd sft-play
./workflows/quick_start.sh
```

---

*Happy fine-tuning! ğŸš€*
