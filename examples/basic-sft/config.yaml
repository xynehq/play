model:
  name: "Qwen/Qwen2.5-1.5B-Instruct"
  max_seq_len: 512
  trust_remote_code: true

tuning:
  mode: "qlora"
  backend: "bnb"
  lora:
    r: 16
    alpha: 32
    dropout: 0.1
    target_modules: "auto"
    bias: "none"
    task_type: "CAUSAL_LM"

train:
  epochs: 3
  learning_rate: 2e-4
  warmup_ratio: 0.06
  weight_decay: 0.01
  batch_size: 4
  grad_accum: 4
  bf16: true
  gradient_checkpointing: true
  
  # Evaluation
  eval_strategy: "steps"
  eval_steps: 50
  predict_with_generate: true
  generation_num_beams: 1
  generation_max_new_tokens: 64
  
  # Saving
  save_strategy: "epoch"
  save_steps: 200
  save_total_limit: 3
  load_best_model_at_end: false
  
  # Logging
  logging_steps: 1
  report_to: ["tensorboard"]
  
  # Data loading
  dataloader_num_workers: 2
  dataloader_pin_memory: true

data:
  train_path: "data/processed/train.jsonl"
  val_path: "data/processed/val.jsonl"
  shuffle_train: true
  seed: 42
