# SFT-Play v2.0: Universal LLM Fine-Tuning CLI

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-red.svg)](https://pytorch.org/)
[![Transformers](https://img.shields.io/badge/ğŸ¤—%20Transformers-4.42+-orange.svg)](https://huggingface.co/transformers/)

**The Universal Command-Line Tool for LLM Fine-Tuning**  
Scale from RTX 4060 to H200 clusters. Train any model, any size, anywhere.

---

## ğŸ¯ **What is SFT-Play?**

SFT-Play is a **production-ready CLI tool** that makes LLM fine-tuning accessible to everyone:

- **ğŸ  Hobbyists**: Fine-tune 7B models on RTX 4060 (8GB)
- **ğŸ“ Researchers**: Experiment with 13B-20B models on RTX 4090 (24GB)  
- **ğŸ¢ Enterprises**: Train 70B+ models on H100/H200 clusters
- **ğŸš€ Everyone**: One tool, any hardware, any model size

## âœ¨ **Key Features**

### **ğŸ”§ Universal Hardware Support**
- **Consumer GPUs**: RTX 4060, 4070, 4080, 4090
- **Professional GPUs**: A100, H100, H200
- **Multi-GPU**: Automatic scaling from 1 to 8+ GPUs
- **Memory Adaptive**: QLoRA, LoRA, Full fine-tuning

### **ğŸ¤– Any Model, Any Size**
- **Small (1-7B)**: Qwen2.5, Gemma-2B, Llama-7B
- **Medium (8-20B)**: Qwen2.5-14B, Llama-13B
- **Large (20-35B)**: Gemma-27B, Qwen-32B
- **Huge (70B+)**: Llama-70B, Qwen-72B

### **ğŸ“ Multiple Training Modes**
- **SFT**: Supervised Fine-Tuning for instruction following
- **DAPT**: Domain-Adaptive Pretraining for specialized knowledge
- **Mixed**: Combine domain expertise with chat capabilities

### **âš¡ Production-Ready CLI**
- **One-Command Setup**: `./workflows/quick_start.sh`
- **Smart Automation**: 25+ Makefile commands
- **Live Monitoring**: TensorBoard integration
- **Error Recovery**: Automatic fallbacks and validation

---

## ğŸš€ **Quick Start (2 Minutes)**

### **Install & Setup**
```bash
git clone <repository-url>
cd experiments/play
make install && make setup-dirs
```

### **Train Your First Model**
```bash
# Interactive setup with sample data
./workflows/quick_start.sh

# Start training with monitoring
make train-bnb-tb

# Chat with your model
make infer
```

### **Scale to Multiple GPUs**
```bash
# One-time setup
make setup-accelerate

# Train large models
make train-distributed-tb CONFIG=configs/run_gemma27b_distributed.yaml
```

---

## ğŸ—ï¸ **Hardware & Model Matrix**

| Your Hardware | VRAM | Recommended Models | Training Mode | Command |
|---------------|------|-------------------|---------------|---------|
| **RTX 4060** | 8GB | Qwen2.5-1.5B, Gemma-2B | QLoRA | `make train-bnb-tb` |
| **RTX 4070** | 12GB | Qwen2.5-3B, Llama-7B | QLoRA | `make train-bnb-tb` |
| **RTX 4080** | 16GB | Qwen2.5-7B, Llama-7B | QLoRA/LoRA | `make train-unsloth-tb` |
| **RTX 4090** | 24GB | Qwen2.5-14B, Llama-13B | QLoRA/LoRA | `make train-unsloth-tb` |
| **A100 40GB** | 40GB | Gemma-27B, Qwen-32B | QLoRA/LoRA | `make train-distributed-tb` |
| **A100 80GB** | 80GB | Llama-70B | LoRA/Full | `make train-distributed-tb` |
| **2x H200** | 282GB | Any model | Full | `make train-deepspeed-tb` |

---

## ğŸ›ï¸ **Command Reference**

### **ğŸ”§ Setup & Validation**
```bash
make install          # Install dependencies (auto-detects pip/uv)
make setup-dirs       # Create project structure
make setup-accelerate # Configure multi-GPU (one-time)
make gpu-info         # Check your hardware
make check            # Validate setup
```

### **ğŸ“Š Data Processing**
```bash
make process          # Raw data â†’ Training format
make style            # Add personality/instructions  
make full-pipeline    # Complete data processing
```

### **ğŸš‚ Single-GPU Training**
```bash
make train-bnb-tb     # BitsAndBytes + TensorBoard (stable)
make train-unsloth-tb # Unsloth + TensorBoard (faster)
make train            # Basic training
```

### **ğŸš€ Multi-GPU Training**
```bash
make train-distributed-tb # Auto-detect GPUs + TensorBoard
make train-deepspeed-tb   # DeepSpeed ZeRO + TensorBoard
make train-distributed    # Basic distributed training
```

### **ğŸ“ˆ Evaluation & Inference**
```bash
make eval             # Evaluate model performance
make infer            # Interactive chat
make infer-batch      # Batch processing
```

### **ğŸ” Monitoring & Debugging**
```bash
make gpu-info         # Hardware information
make memory-check     # GPU memory usage
make tensorboard      # Start TensorBoard manually
```

---

## ğŸ“ **Project Structure**

```
sft-play/
â”œâ”€â”€ ğŸ“‹ GETTING_STARTED.md     # Complete beginner's guide
â”œâ”€â”€ ğŸš€ MULTI_GPU_GUIDE.md     # Multi-GPU training guide
â”œâ”€â”€ âš™ï¸ configs/               # Training configurations
â”‚   â”œâ”€â”€ run_bnb.yaml          # Single GPU (stable)
â”‚   â”œâ”€â”€ run_unsloth.yaml      # Single GPU (fast)
â”‚   â”œâ”€â”€ run_gemma27b_distributed.yaml # Large models
â”‚   â””â”€â”€ deepspeed_z2.json     # DeepSpeed configuration
â”œâ”€â”€ ğŸ¤– scripts/               # Core training scripts
â”‚   â”œâ”€â”€ train.py              # Single-GPU training
â”‚   â”œâ”€â”€ train_distributed.py  # Multi-GPU training
â”‚   â”œâ”€â”€ eval.py               # Model evaluation
â”‚   â””â”€â”€ infer.py              # Interactive inference
â”œâ”€â”€ ğŸ“Š data/                  # Training data
â”œâ”€â”€ ğŸ”§ workflows/             # Automation scripts
â””â”€â”€ ğŸ“ˆ outputs/               # Training results & TensorBoard logs
```

---

## ğŸ¯ **Use Cases & Examples**

### **ğŸ  Hobbyist: Custom Assistant (RTX 4060)**
```bash
# Train a 1.5B personal assistant
echo '{"user": "What is machine learning?", "assistant": "Machine learning is..."}' > data/raw/qa.jsonl
make process && make train-bnb-tb
```

### **ğŸ“ Researcher: Domain Expert (RTX 4090)**
```bash
# Create a medical expert model
# 1. Add medical papers to data/raw/
make dapt-docx && make dapt-train
```

### **ğŸ¢ Enterprise: Large Model (2x H200)**
```bash
# Train Gemma 27B for production
make setup-accelerate
make train-deepspeed-tb CONFIG=configs/run_gemma27b_distributed.yaml
```

---

## ğŸ§  **Training Modes Explained**

### **QLoRA (Recommended)**
- **Memory**: 4-bit quantization, ~75% memory reduction
- **Quality**: 95%+ of full fine-tuning performance
- **Use Case**: Large models on small GPUs

### **LoRA**
- **Memory**: 16-bit base model + small adapters
- **Quality**: 98%+ of full fine-tuning performance  
- **Use Case**: Balanced memory/performance

### **Full Fine-tuning**
- **Memory**: Full model in memory
- **Quality**: Maximum possible performance
- **Use Case**: Enterprise hardware, best results

---

## ğŸ“Š **Real Performance Results**

### **Proven Results**
- **10-minute QLoRA**: ROUGE-L improved from 0.17 â†’ 0.33 (+95%)
- **Gemma 27B on 2x H200**: ~130GB per GPU, 1.8x speedup
- **Memory Efficiency**: Train 70B models on 2x A100 (160GB total)

### **Scaling Efficiency**
- **Single â†’ Multi-GPU**: Near-linear speedup for large models
- **Memory Scaling**: Linear VRAM pooling across GPUs
- **Cost Efficiency**: Train enterprise models on consumer hardware

---

## ğŸ”§ **Advanced Features**

### **ğŸ¤– Automatic Optimization**
- **Batch Size**: Auto-calculated based on available VRAM
- **Memory Management**: Smart gradient checkpointing
- **Backend Selection**: Automatic fallbacks for compatibility

### **ğŸ” Production Monitoring**
- **TensorBoard**: Live training metrics and GPU utilization
- **Error Recovery**: Automatic checkpoint resuming
- **Validation**: Comprehensive setup checking

### **âš¡ Performance Optimizations**
- **DeepSpeed Integration**: ZeRO optimizer for maximum efficiency
- **Mixed Precision**: BF16/FP16 for modern GPUs
- **Data Pipeline**: Optimized data loading and processing

---

## ğŸ› ï¸ **Installation & Dependencies**

### **System Requirements**
- **Python**: 3.8+ (3.10+ recommended)
- **CUDA**: 11.8+ for GPU training
- **Memory**: 16GB+ RAM recommended
- **Storage**: 50GB+ for models and data

### **Automatic Installation**
```bash
make install  # Auto-detects pip or uv
```

### **Manual Installation**
```bash
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
pip install transformers accelerate deepspeed peft datasets evaluate
pip install tensorboard jinja2 pyyaml
```

---

## ğŸ“š **Documentation**

- **ğŸ“‹ [GETTING_STARTED.md](GETTING_STARTED.md)** - Complete beginner's guide
- **ğŸš€ [MULTI_GPU_GUIDE.md](MULTI_GPU_GUIDE.md)** - Multi-GPU training guide  
- **âš™ï¸ [AUTOMATION_GUIDE.md](AUTOMATION_GUIDE.md)** - Advanced automation
- **ğŸ”§ [SETUP_DOCUMENTATION.md](SETUP_DOCUMENTATION.md)** - Detailed setup

---

## ğŸ¤ **Community & Support**

### **Getting Help**
```bash
make help                    # Built-in command reference
./workflows/quick_start.sh   # Interactive setup guide
make check                   # Validate your setup
```

### **Contributing**
- **Issues**: Report bugs and request features
- **Pull Requests**: Contribute improvements
- **Documentation**: Help improve guides
- **Examples**: Share your use cases

---

## ğŸ‰ **What's New in v2.0**

### **ğŸš€ Multi-GPU Support**
- Distributed training across any number of GPUs
- DeepSpeed integration for maximum memory efficiency
- Automatic hardware detection and optimization

### **ğŸ“ˆ Scalability**
- Train models from 1.5B to 70B+ parameters
- Support for enterprise hardware (H100, H200)
- Linear scaling across GPU clusters

### **ğŸ›ï¸ Enhanced CLI**
- 25+ automation commands
- Smart configuration management
- Production-ready error handling

### **ğŸ§  Advanced Training**
- Domain-Adaptive Pretraining (DAPT)
- Mixed training modes
- Custom model architectures

---

## ğŸ† **Why Choose SFT-Play?**

### **ğŸ¯ Universal**
One tool that scales from hobbyist RTX 4060 to enterprise H200 clusters

### **ğŸš€ Production-Ready**
Battle-tested automation, error recovery, and monitoring

### **ğŸ§  Intelligent**
Automatic optimization, smart memory management, adaptive scaling

### **ğŸ“š Beginner-Friendly**
Comprehensive guides, interactive setup, clear documentation

### **âš¡ Performance-Focused**
State-of-the-art optimizations, multi-GPU scaling, memory efficiency

---

## ğŸš€ **Get Started Now**

```bash
# Clone and setup (2 minutes)
git clone <repository-url>
cd experiments/play
make install && make setup-dirs

# Interactive first-time setup
./workflows/quick_start.sh

# Start training immediately
make train-bnb-tb
```

**Ready to fine-tune your first model?** ğŸ¯

---

## ğŸ“„ **License**

MIT License - see [LICENSE](LICENSE) for details.

---

**SFT-Play v2.0**: From 8GB to 800GB. From hobbyist to enterprise. One tool, infinite possibilities. ğŸš€
