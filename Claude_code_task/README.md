# PR Benchmarking Automation System

## Overview

This automation system benchmarks how well different AI models can implement real PR fixes when given only a natural-language task description. It uses a strict separation between reasoning (LLM), execution (Claude Code), and evaluation (LLM).

### Key Principle

**Claude Code NEVER thinks. Claude Code ONLY executes.**

- **LLM #1** → Generates execution prompt
- **Claude Code** → Applies code changes
- **LLM #2** → Evaluates output

No shortcuts. No coupling.

---

## Table of Contents

1. [Architecture](#architecture)
2. [Prerequisites](#prerequisites)
3. [Quick Start](#quick-start)
4. [Stage-by-Stage Flow](#stage-by-stage-flow)
5. [Command Reference](#command-reference)
6. [Artifact Structure](#artifact-structure)
7. [Troubleshooting](#troubleshooting)

---

## Architecture

### Components

```
┌─────────────────┐
│   run_pr CLI    │  Entry point: `python3 run_pr <PR_NUMBER> --models <MODEL>`
└────────┬────────┘
         │
         ▼
┌─────────────────┐
│  orchestrator   │  Main control flow
└────────┬────────┘
         │
         ├──► Prompt Generation (LLM: minimaxai/minimax-m2)
         ├──► Claude Code Execution (Model: specified via --models)
         └──► Evaluation (LLM: same as execution model)
```

### Core Invariants (Must Never Break)

1. ✅ **Prompts are ALWAYS generated by LLM, never handwritten**
2. ✅ **Claude Code NEVER sees human diff** (only sees generated prompts)
3. ✅ **Git state is reset before every attempt**
4. ✅ **Base commit is `merge_commit^1`** (one commit before PR merge)
5. ✅ **Evaluation is diff-based and model-agnostic**
6. ✅ **Prompt generation ALWAYS uses `minimaxai/minimax-m2`** (regardless of execution model)
7. ✅ **Stop immediately on PASS**
8. ✅ **Stop after 3 attempts regardless of outcome**

---

## Prerequisites

### Required Tools

1. **Python 3.7+**
   ```bash
   python3 --version
   ```

2. **Git** (with access to the repository)
   ```bash
   git --version
   ```

3. **GitHub CLI (`gh`)** - For PR metadata fetching
   ```bash
   gh --version
   ```

4. **Claude Code CLI** - For code execution
   ```bash
   claude --version
   ```

### Stage 0 Automation

**Stage 0 is now fully automated!** The system will automatically:
- Fetch PR metadata via GitHub CLI
- Create isolated branch
- Generate task description and ground-truth diff

**Note:** If you prefer manual setup, you can still use `pr_analyzer.sh` and add the `--skip-setup` flag when running automation.

### Environment Setup

The automation uses these API endpoints:
- **Base URL**: `https://grid.ai.juspay.net`
- **API Keys**: Configured in `orchestrator.py`

---

## Quick Start

### Automated Workflow (Recommended)

**Single command - Stage 0 runs automatically:**

```bash
cd /path/to/claude-work
./automation_2/run_pr <PR_NUMBER> --models <MODEL_NAME>
```

**Example:**
```bash
./automation_2/run_pr 8916 --models glm-full
```

**Multiple models:**
```bash
./automation_2/run_pr 8916 --models glm-full,minimax-m2
```

This will:
1. **Automatically fetch PR metadata** from GitHub
2. **Checkout base commit** (`merge_commit^1`)
3. **Generate task description** (`pr/task_pr_<PR>.md`)
4. **Generate ground-truth diff** (`pr/original_changes.diff`)
5. **Run full automation** (Stages 1-8)

### Manual Workflow (Legacy)

If you prefer manual setup or need custom issue context:

**Step 1: Setup PR manually**
```bash
./automation_2/pr_analyzer.sh <PR_NUMBER>
```

**Step 2: Run automation (skip Stage 0)**
```bash
./automation_2/run_pr <PR_NUMBER> --models <MODEL_NAME> --skip-setup
```

### Check Results

Results are saved in:
```
automation_2/runs/PR-<PR_NUMBER>/<model>/
├── final.json          # Final verdict
├── p1/                 # Attempt 1 artifacts
├── p2/                 # Attempt 2 artifacts (if needed)
└── p3/                 # Attempt 3 artifacts (if needed)
```

---

## Stage-by-Stage Flow

### Stage 0: PR Setup (Automated)

**Module**: `pr_setup.py`

**Purpose**: Prepare PR data for automation

**Status**: ✅ **Fully Automated** (runs automatically at the start of pipeline)

**Actions**:
1. Fetch PR metadata via `gh pr view <PR_NUMBER> --json title,body,state,mergeCommit`
2. Verify PR is merged (fail fast if not)
3. Identify merge commit from GitHub API
4. Compute base commit as `merge_commit^1`
5. Verify commit exists locally (fetch from upstream if needed)
6. Create isolated branch `test-claude-pr-<PR_NUMBER>` (fail if exists)
7. Generate `pr/task_pr_<PR>.md` (natural language task description)
8. Generate `pr/original_changes.diff` (ground truth diff via `git diff BASE MERGE`)
9. Verify outputs are non-empty (fail if empty)
10. Verify clean git state

**Outputs**:
- `pr/task_pr_<PR>.md` - Task description
- `pr/original_changes.diff` - Human fix (ground truth)

**Automation**:
```bash
# Automatic (default)
./automation_2/run_pr <PR_NUMBER>

# Manual override (legacy)
./automation_2/pr_analyzer.sh <PR_NUMBER>
./automation_2/run_pr <PR_NUMBER> --skip-setup
```

**Error Handling**:
- Fails fast if PR not merged
- Fails fast if branch already exists
- Fails fast if diff is empty
- Fails fast if commits not found
- Clear error messages for all failures

**Documentation**: See `STAGE_0_AUTOMATION.md` for complete details

---

### Stage 1: Initialization

**Function**: `run_pr()` in `orchestrator.py`

**Purpose**: Load inputs and generate shared artifacts

**Actions**:
1. Load PR description from `pr/task_pr_<PR>.md`
2. Load human diff from `pr/original_changes.diff`
3. Extract PR metadata (title, description, issue)
4. Load prompt templates from `Prompt.md`
5. Generate human approach summary (LLM call using `minimaxai/minimax-m2`)

**Outputs**:
- `runs/PR-<PR>/human_approach.txt` - Human approach summary (shared)
- Human approach summary (in memory, used for evaluation)

**LLM Used**: `minimaxai/minimax-m2` (for human approach generation)

---

### Stage 2: Model Loop

**Function**: `run_pr()` → Model iteration

**Purpose**: Run automation for each specified model

**Flow**:
```
For each model in --models:
    ├── Create model-specific directory
    ├── Copy inputs to input/
    ├── Generate human approach (if not already done)
    └── Run attempt loop (max 3 attempts)
```

**Outputs**:
- `runs/PR-<PR>/<model>/input/` - Input files for this model
- `runs/PR-<PR>/<model>/final.json` - Final verdict for this model

---

### Stage 3: Attempt Loop (Max 3 Attempts)

**Function**: `_run_attempts_loop()`

**Purpose**: Try up to 3 times to get a PASS

**Flow**:
```
Attempt 1:
    ├── Generate P1 prompt
    ├── Execute Claude Code
    ├── Capture diff
    ├── Evaluate
    └── PASS? → Stop, else continue

Attempt 2 (if Attempt 1 failed):
    ├── Generate P2 prompt (with P1 feedback)
    ├── Execute Claude Code
    ├── Capture diff
    ├── Evaluate
    └── PASS? → Stop, else continue

Attempt 3 (if Attempt 2 failed):
    ├── Generate P3 prompt (with P2 feedback)
    ├── Execute Claude Code
    ├── Capture diff
    ├── Evaluate
    └── PASS/FAIL → Stop (hard limit)
```

**Stopping Conditions**:
- ✅ **PASS on any attempt** → Stop immediately
- ✅ **3 attempts completed** → Stop regardless of outcome

---

### Stage 4: Prompt Generation (Per Attempt)

**Function**: `_run_single_attempt()` → Prompt generation section

**Purpose**: Generate execution prompt for Claude Code

#### Attempt 1 (P1)

**Inputs**:
- PR description (`task_pr_<PR>.md`)
- Human diff (`original_changes.diff`)
- **NO Claude diff** (first attempt)

**Template**: `templates.prompt1` from `Prompt.md`

**LLM**: `minimaxai/minimax-m2` (ALWAYS, regardless of execution model)

**Output**: `p1/p1_prompt.txt`

**Example Prompt**:
```
You are tasked with fixing a bug in the authentication module where 
terminal state is not being persisted when sending redirect responses...
```

#### Attempt 2 (P2)

**Inputs**:
- PR description
- Human diff
- **Claude P1 diff** (previous attempt)
- **P1 evaluation feedback** (what went wrong)

**Template**: `templates.prompt2` from `Prompt.md`

**LLM**: `minimaxai/minimax-m2` (ALWAYS)

**Output**: `p2/p2_prompt.txt`

**Purpose**: Fix what Claude missed, preserve correct changes

#### Attempt 3 (P3)

**Inputs**:
- PR description
- Human diff
- **Claude P2 diff** (previous attempt)
- **P2 evaluation feedback** (what went wrong)

**Template**: `templates.prompt3` from `Prompt.md`

**LLM**: `minimaxai/minimax-m2` (ALWAYS)

**Output**: `p3/p3_prompt.txt`

**Purpose**: Final corrective attempt, minimal targeted fixes

**Critical Rule**: Prompt generation ALWAYS uses `minimaxai/minimax-m2`, never the execution model.

---

### Stage 5: Repository Reset

**Function**: `_reset_repository()`

**Purpose**: Ensure clean state before Claude Code execution

**Actions**:
1. `git reset --hard` (reset to HEAD)
2. `git clean -fd` (remove untracked files)

**When**: Before every Claude Code execution (every attempt)

**Why**: Prevent contamination from previous attempts

---

### Stage 6: Claude Code Execution

**Function**: `_execute_claude_code()`

**Purpose**: Execute code changes based on generated prompt

**Execution Contract**:
```bash
ANTHROPIC_BASE_URL="https://grid.ai.juspay.net" \
ANTHROPIC_AUTH_TOKEN="sk-uJfk3pIE2KcP9DoGx4UeHA" \
claude --model "<MAPPED_MODEL>" --print --permission-mode acceptEdits
```

**Model Mapping**:
- `glm-full` → `glm-full` (as-is)
- `minimax-m2` → `minimaxai/minimax-m2`

**Actions**:
1. Read prompt from `p<N>_prompt.txt`
2. Clean prompt (remove `<think>` tags if present)
3. Set environment variables (`ANTHROPIC_BASE_URL`, `ANTHROPIC_AUTH_TOKEN`)
4. Execute Claude Code with prompt via stdin
5. Capture stdout → `p<N>/stdout.txt`
6. Capture stderr → `p<N>/stderr.txt`

**Outputs**:
- `p<N>/stdout.txt` - Claude Code output
- `p<N>/stderr.txt` - Error output (if any)

**Working Directory**: `claude-work` (parent of `automation_2`)

**Timeout**: 10 minutes per execution

---

### Stage 7: Diff Capture

**Function**: `_capture_git_diff()`

**Purpose**: Capture code changes made by Claude Code

**Actions**:
1. Run `git diff` in `claude-work` directory
2. Save to `p<N>/claude.diff`
3. Log if diff is empty (warning)

**Outputs**:
- `p<N>/claude.diff` - Git diff of changes

**Empty Diff Handling**:
- If diff is empty → Log warning: "Claude Code made no changes"
- Empty diff = Claude Code didn't modify any files

---

### Stage 8: Evaluation

**Function**: `_evaluate_solution()`

**Purpose**: Compare Claude Code's changes with human fix

**Inputs**:
- PR metadata (title, description, issue)
- Human approach summary
- Ground truth diff (`original_changes.diff`)
- Model diff (`claude.diff`)

**LLM Call**:
- **Model**: Execution model (same as Claude Code execution)
- **Template**: `templates.evaluation` from `Prompt.md`
- **Purpose**: Determine if model's changes match human fix

**Evaluation Rules** (from `Prompt.md`):

**PASS when**:
- Changes are logically equivalent to human fix
- Semantically aligned, same intent
- Minor differences are acceptable (names, refactors)
- No harmful behavior introduced

**FAIL when**:
- Changes don't solve core problem
- Different strategy that doesn't address bug
- Missing core fix logic
- Introduces incorrect/dangerous behavior

**Outputs**:
- `p<N>/eval.json` - Contains `verdict` (PASS/FAIL) and `reason`

**Example eval.json**:
```json
{
  "verdict": "PASS",
  "reason": "Summary\nClaude Code correctly identified the core problem..."
}
```

---

### Stage 9: Retry Decision

**Function**: `_run_attempts_loop()`

**Logic**:
```
If verdict == "PASS":
    → Stop immediately
    → Return PASS
    → Set passed_on = attempt_number

Else if attempts < 3:
    → Continue to next attempt
    → Generate next prompt with feedback

Else (attempts == 3):
    → Stop (hard limit)
    → Return FAIL
```

---

### Stage 10: Final Results

**Function**: `run_pr()`

**Purpose**: Generate final reports

**Outputs**:

**Per Model**:
- `runs/PR-<PR>/<model>/final.json`

**Example final.json**:
```json
{
  "pr": "8916",
  "final_verdict": "PASS",
  "attempts": 2,
  "passed_on": 2
}
```

**Multi-Model Comparison**:
- `runs/PR-<PR>/model_comparison.json` (if multiple models)

**Example model_comparison.json**:
```json
{
  "glm-full": {
    "pr": "8916",
    "final_verdict": "PASS",
    "attempts": 2,
    "passed_on": 2
  },
  "minimax-m2": {
    "pr": "8916",
    "final_verdict": "FAIL",
    "attempts": 3,
    "passed_on": null
  }
}
```

---

## Command Reference

### Run Automation (Recommended)

**Automatic Stage 0 + Full Pipeline**:
```bash
cd /path/to/claude-work
./automation_2/run_pr <PR_NUMBER> --models <MODEL>
```

**Single Model**:
```bash
./automation_2/run_pr 8916 --models glm-full
```

**Multiple Models**:
```bash
./automation_2/run_pr 8916 --models glm-full,minimax-m2
```

**Skip Stage 0** (if you already ran manual setup):
```bash
./automation_2/run_pr 8916 --models glm-full --skip-setup
```

**View Help**:
```bash
./automation_2/run_pr --help
```

---

### Manual PR Setup (Legacy)

**Only use if you need custom issue context or prefer manual control:**

```bash
cd /path/to/claude-work
./automation_2/pr_analyzer.sh <PR_NUMBER>
```

**What it does**:
- Fetches PR metadata interactively
- Allows custom issue linking
- Checks out base commit
- Generates task file and diff

**Then run with --skip-setup**:
```bash
./automation_2/run_pr <PR_NUMBER> --models <MODEL> --skip-setup
```

**Model Names**:
- `glm-full` → Uses `glm-full` for execution
- `minimax-m2` → Maps to `minimaxai/minimax-m2` for execution
- Prompt generation always uses `minimaxai/minimax-m2` (regardless of execution model)

---

### Check Results

**View final verdict**:
```bash
cat automation_2/runs/PR-8916/glm-full/final.json
```

**View attempt 1 prompt**:
```bash
cat automation_2/runs/PR-8916/glm-full/p1/p1_prompt.txt
```

**View attempt 1 diff**:
```bash
cat automation_2/runs/PR-8916/glm-full/p1/claude.diff
```

**View attempt 1 evaluation**:
```bash
cat automation_2/runs/PR-8916/glm-full/p1/eval.json
```

**View human approach**:
```bash
cat automation_2/runs/PR-8916/human_approach.txt
```

---

## Artifact Structure

### Complete Directory Tree

```
automation_2/
├── pr/
│   ├── task_pr_8916.md          # PR task description (Stage 0)
│   └── original_changes.diff     # Ground truth diff (Stage 0)
│
├── runs/
│   └── PR-8916/
│       ├── human_approach.txt     # Human approach summary (shared)
│       ├── model_comparison.json   # Multi-model comparison (if multiple models)
│       │
│       └── glm-full/              # Model-specific run
│           ├── input/
│           │   ├── pr.md              # Copied PR description
│           │   ├── human.diff         # Copied ground truth
│           │   └── human_approach.txt # Human approach summary
│           │
│           ├── p1/                    # Attempt 1
│           │   ├── p1_prompt.txt      # Generated prompt (LLM output)
│           │   ├── claude.diff         # Claude Code changes
│           │   ├── eval.json          # Evaluation result
│           │   ├── stdout.txt         # Claude Code stdout
│           │   └── stderr.txt         # Claude Code stderr
│           │
│           ├── p2/                    # Attempt 2 (if needed)
│           │   └── ... (same structure)
│           │
│           ├── p3/                    # Attempt 3 (if needed)
│           │   └── ... (same structure)
│           │
│           └── final.json            # Final verdict
│
└── orchestrator.py                   # Main automation logic
```

### Key Files Explained

| File | Purpose | Generated By |
|------|---------|--------------|
| `pr/task_pr_<PR>.md` | Natural language task description | `pr_analyzer.sh` |
| `pr/original_changes.diff` | Ground truth (human fix) | `pr_analyzer.sh` |
| `runs/PR-<PR>/human_approach.txt` | Human approach summary | LLM (minimaxai/minimax-m2) |
| `p<N>/p<N>_prompt.txt` | Execution prompt for Claude Code | LLM (minimaxai/minimax-m2) |
| `p<N>/claude.diff` | Code changes made by Claude Code | `git diff` after execution |
| `p<N>/eval.json` | PASS/FAIL verdict with reason | LLM (execution model) |
| `final.json` | Final verdict and attempt count | Orchestrator |

---

## Troubleshooting

### Issue: Empty `claude.diff`

**Symptoms**: `claude.diff` is empty (0 bytes)

**Possible Causes**:
1. Claude Code didn't execute (check `stderr.txt`)
2. Claude Code executed but made no changes
3. Git diff failed (check working directory)

**Debug Steps**:
```bash
# Check stderr
cat automation_2/runs/PR-8916/glm-full/p1/stderr.txt

# Check stdout
cat automation_2/runs/PR-8916/glm-full/p1/stdout.txt

# Verify git state
cd /path/to/claude-work
git status
git diff
```

**Fix**: Ensure Claude Code execution is working (see test below)

---

### Issue: Evaluation PASS with Empty Diff

**Symptoms**: `eval.json` shows PASS but `claude.diff` is empty

**Cause**: Evaluation LLM incorrectly passed empty diff

**Status**: This is a known bug - empty diffs should auto-FAIL

**Workaround**: Check `claude.diff` manually - if empty, ignore PASS verdict

---

### Issue: Claude Code Not Executing

**Symptoms**: `stdout.txt` shows "No response requested" or error

**Debug Steps**:
```bash
# Test Claude Code execution manually
cd /path/to/claude-work
ANTHROPIC_BASE_URL="https://grid.ai.juspay.net" \
ANTHROPIC_AUTH_TOKEN="sk-uJfk3pIE2KcP9DoGx4UeHA" \
echo "Add a comment" | claude --model glm-full --print --permission-mode acceptEdits
```

**Fix**: Ensure:
- Claude Code CLI is installed and working
- Environment variables are set correctly
- Model name is correct

---

### Issue: Prompt Generation Includes Reasoning Tags

**Symptoms**: `p1_prompt.txt` contains `<think>` or `<think>` tags

**Status**: This is handled automatically - prompts are cleaned before execution

**Fix**: Already fixed in code - reasoning tags are removed before passing to Claude Code

---

### Issue: Wrong Model Used for Prompt Generation

**Symptoms**: Prompt generation uses execution model instead of `minimaxai/minimax-m2`

**Fix**: This should never happen - prompt generation ALWAYS uses `minimaxai/minimax-m2`

**Verify**:
```bash
# Check orchestrator.py - should have:
prompt_gen_client = LLMClient(
    model="minimaxai/minimax-m2",  # Always use this
    ...
)
```

---

### Issue: Branch Already Exists

**Symptoms**: Stage 0 fails with "Branch 'test-claude-pr-XXXX' already exists"

**Cause**: The branch was created in a previous run and wasn't cleaned up

**Fix**: Delete the existing branch and re-run:
```bash
cd /path/to/claude-work

# If the branch is currently checked out, checkout another branch first
git checkout main  # or any other branch

# Delete the test branch
git branch -D test-claude-pr-9298

# Re-run automation
./automation_2/run_pr 9298 --models minimax-m2
```

**Alternative**: Use `--skip-setup` if PR setup was already completed:
```bash
# Branch already exists but setup is done
./automation_2/run_pr 9298 --models minimax-m2 --skip-setup
```

---

### Issue: Cannot Delete Branch (Worktree In Use)

**Symptoms**: `error: cannot delete branch 'test-claude-pr-XXXX' used by worktree`

**Cause**: You're currently on the branch you're trying to delete

**Fix**: Switch to a different branch first:
```bash
cd /path/to/claude-work

# Check current branch
git branch --show-current

# If on test-claude-pr-XXXX, use --skip-setup instead
./automation_2/run_pr 9298 --models minimax-m2 --skip-setup
```

---

### Issue: Git Reset Not Working

**Symptoms**: Previous attempt's changes persist

**Debug Steps**:
```bash
cd /path/to/claude-work
git status
git diff
```

**Fix**: Ensure you're in the correct directory (`claude-work`, not `automation_2`)

---

## Best Practices

1. **Always run Stage 0 first** (`pr_analyzer.sh`) before automation
2. **Check base commit** - Ensure you're on `merge_commit^1`
3. **Verify inputs exist** - Check `pr/task_pr_<PR>.md` and `pr/original_changes.diff`
4. **Monitor execution** - Watch stdout/stderr for errors
5. **Review diffs** - Always check `claude.diff` manually, don't trust eval alone
6. **Clean state** - Ensure git is clean before running automation

---

## Example Workflows

### Automated Workflow (Recommended): PR 8916 with glm-full

```bash
# Step 1: Run automation (Stage 0 runs automatically)
cd /Users/aditya.singh.001/Desktop/hyperswitch/claude-work
./automation_2/run_pr 8916 --models glm-full

# Step 2: Check results
cat automation_2/runs/PR-8916/glm-full/final.json

# Step 3: Review artifacts
cat automation_2/runs/PR-8916/glm-full/p1/claude.diff
cat automation_2/runs/PR-8916/glm-full/p1/eval.json

# Step 4: Review generated inputs (Stage 0 outputs)
cat automation_2/pr/task_pr_8916.md
cat automation_2/pr/original_changes.diff
```

### Multi-Model Benchmark

```bash
# Run same PR with multiple models
./automation_2/run_pr 8916 --models glm-full,minimax-m2

# Compare results
cat automation_2/runs/PR-8916/model_comparison.json
```

### Manual Workflow (Legacy): PR 8916 with custom setup

```bash
# Step 1: Manual setup with custom issue context
cd /Users/aditya.singh.001/Desktop/hyperswitch/claude-work
./automation_2/pr_analyzer.sh 8916

# Step 2: Verify inputs
ls -la automation_2/pr/task_pr_8916.md
ls -la automation_2/pr/original_changes.diff

# Step 3: Run automation (skip Stage 0)
./automation_2/run_pr 8916 --models glm-full --skip-setup

# Step 4: Check results
cat automation_2/runs/PR-8916/glm-full/final.json
```

### Cleanup Old Test Branches

```bash
# If you get "branch already exists" error
git branch -D test-claude-pr-8916

# Then re-run automation
./automation_2/run_pr 8916 --models glm-full
```

---

## Summary

This automation system provides a reproducible, fair, and auditable benchmark to measure how well different AI models can implement real PR fixes when given only a natural-language task description.

**Key Features**:
- ✅ Strict separation: LLM → reasoning, Claude Code → execution, LLM → evaluation
- ✅ Up to 3 attempts with retry logic
- ✅ Multi-model support
- ✅ Complete artifact tracking
- ✅ Diff-based evaluation

**Critical Invariants**:
- Prompts are always LLM-generated
- Claude Code never sees human diff
- Git reset before every attempt
- Prompt generation always uses `minimaxai/minimax-m2`

For questions or issues, refer to the troubleshooting section or check the code comments in `orchestrator.py`.
